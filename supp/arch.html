<HTML>
<HEAD>
<TITLE>Architecture Does All That?</TITLE>
</HEAD>
<BODY>
<CENTER>
<H1>Architecture Does All That?</H1>
	Mark Kampe
</CENTER>
<P>

<H2>1. Introduction</h2>

A software product is more likely to work if it is designed before it is built.
If the product is comprised of multiple interacting components, we also have to
design those interactions.  If all we cared about was functionality, 
architecture would be a simple matter of finding a set of component
definitions that could (working in concert) deliver the required 
functionality ... and there are surely numerous viable approaches.
<P>
As you have probably begun to recognize, we often need a great deal more
than <em>run-time functionality</em> from our software ... and it turns out that
most of those other things are also driven (even more than functionality)
by architecture.  As a result, the architecture of an interesting system
is much more than <em>a realization of the requirements</em>,
and the problem of finding a viable architecture becomes much more difficult
... but being aware of those other goals greatly increases our chances
of achieving them.
<P>
This paper is a brief overview of the other kinds of problems that
architecture may be required to solve.
<P>

<H2>2. Architecture and Performance/Reliability/Availability</h2>
<P>
One might guess that performance is mostly a matter of 
<em>"algorithmic efficiency"</em>.  Poor 
choices 
(e.g. using an n<sup>2</sup> search rather than a log<sub>n</sub> search) 
can certainly create problems.  But many (if not most) performance problems
are caused by architectural decisions.  A few examples
are:
<UL>
   <LI> the number of layers through which a request
	must be processed.</li>
   <LI>	directing a high volume of traffic through a 
	single low-bandwidth component or channel.</li>
   <LI>	the number of messages that have to be exchaged
	to perform an operation.</li>
   <LI> important operations that complete slowly because 
	they must wait in long lines.</li>
</UL>
<P>
Similarly one might guess that reliability and availability
were the result of well reviewed and tested code.  Again,
poorly reviewed and tested code can certainly create 
reliability/availability problems ... but simple bugs are 
not the only causes of system failures:
<UL>
   <LI> disks and memory are notoriously unreliable.</li>
   <LI> computers and network links go down regularly.</li>
   <LI> the software with which we interact may behave badly.</li>
   <LI> many failures are the results of external factors
	like power outages and operator errors.</li>
</UL>
To be robust in the face of such failure:
<UL>
   <LI> alternates and recovery procedures must exist for every component.</li>
   <LI> component interfaces must prevent the failure of one component
	from triggering a cascade of secondary failures.</li>
   <LI> component interfaces must be designed so that any component
	can be replaced or restarted at any time, while the system
	continues to operate.</li>
</UL>
These capabilities arise, not from careful coding, 
but from the architecture.
<P>
<H2>3. Architecture and Construction</h2>
<P>
If a system is to be buildable with existing tools and 
skills, then each component must be specified to 
be implementable within the limitations of those 
skills and tools.  This may greatly limit the range
of viable component specifications.
</P>
<P>
If a system is to be buildable using off-the-shelf technology,
the interfaces to those components must be defined to match
those of the available technology.  If a system is expected
to create new reusable components, the interfaces to those
components must be designed to meet the needs of future
clients as well as the current one.
</P>
<P>
If we want to enable independent, parallel development of
distinct components their interfaces must sufficiently well
abstracted as to permit them to be designed independently,
and sufficiently simple and well defined that each can be 
tested (against the specifications or with simulators)
for interoperability before the other is available.
</P>
<P>
If we want to enable continuous integration, each component
must have functional interfaces that are easily stubbed or
simulated until more complete implementations are available,
and the details of those interfaces may well determine the
order in which specific features must be implemented.
</P>
<P>
If we want to be able to gain confidence about the correctness
of a component implementation, the component interface
specifications must include the ability to generate all 
behavioral scenarios, and definitively ascertain the correctness 
of the components behavior in every case.
</P>
<P>
All of these process requirements are enabled (or condemned to failure)
by the system architecture.
The construction and integration process for a large project
must be archtitected just as carefully as the run-time
components ... and those two architectures must agree with
one-another.
</P>
<H2>4. Architecture and Diagnosibility/Servicability</h2>
<P>
When a system misbehaves in the field, it should be possible
to diagnose all likely errors (or at least isolate them to 
a particular component) by looking at a small number of
control points.  When we think we have isolated the failure
to a particular component, it should be possible to confirm
this diagnosis by sending test operations through the
component in question.  This will only be possible if the 
components and their interfaces were defined with such 
diagnostic procedures in mind.
<P>
When a failure has been diagnosed to a particular component, 
it should be possible to reset, restart, replace, or update
that component without reinstalling the entire system.  
If a system is to support such incremental repairs, all
of the components must have been designed with these procedures
in mind.
<P>
All of these characteristics arise from the architecture.
<P>
<H2>5. Architecture and Evolution</h2>
<P>
Few programs are "write and forget".
We will be adding new features to them and adapting them to
exploit new platforms and to be used in new ways.  If the software
has been designed with consideration given to likely changes,
these future extensions may be easy.  If not, they may
be impossible (i.e. extending it will be more expensive than
throwing it away and starting from scratch).
Unfortunately, 
as a great commentator on the human condition
(Yogi Berra) once observed:
<UL><em>
	Prediction is very hard,<br>
	especially about the the future.
</em></UL>
<P>
How can we predict what kinds of change will be necessary?
Fortunately, many types of change are predictable:
<UL>
   <LI> recognized limitations we didn't have time to fix.</li>
   <LI> poorly implemented features that will need later improvement.</li>
   <LI> features we left out for want of time.</li>
   <LI> features we left out because they are not yet well defined, or
   	we weren't yet sure how to implement them.</li>
   <LI> additional plug-in extensions
	(e.g. support for additional MIME types or authentication methods)
        that are likely to be added later.</li>
   <LI> features that are not yet required, but will likely become
	required later (e.g. internationalization/localization).</li>
   <LI> policy decisions that might need to be made differently in the future
	(e.g. role based access control).</li>
   <LI> ports to other platforms (e.g. Android and IOS).</li>
</UL>
All of these should already be well known to the developers.
Even if we don't have time to create those implementations
at this time, we can consider what a future implementations
might look like.  We could start by looking (individually) at 
each such extension and asking what the scope of the required changes
might be:
<ul>
   <li> ideally the changes (for a particular extension) would be
   	localized within a single component, and the rest of the
	system would not see any difference.</li>
   <li> also acceptable, most of the implementation would still
   	be primarily upwards-compatible extensions to a single component,
	but a few other components might have to be extended to use
	or support new options.</li>
</ul>
But, if the changes appeared to be wide-spread (or worse, not achievable within
this architecture) we should explore different architectures that might be
more robust in the face of such extensions:
<UL>
   <LI> if mechanisms are likely to change, define more abstracted 
	interfaces that can encompass a wider range of implementations.</li>
   <LI> if underlying services are likely change, encapsulate them
	in a layer of abstraction hide the differences.</li>
   <LI> if alternative implementations of something are likely to be
	needed, create plug-in interfaces to enable the addition of 
	new providers (even at run-time).</li>
   <LI> if smarter implementations are likely to be required in the
	future, design the interfaces to support smarter services, and
	then provide degenerate (e.g. hard-coded) implementations 
	to provide simple default behavior in the first release.</li>
</UL>
<P>
It is good to consider where more general abstractions are likely
to be important ... but this can be a trap.  Creating layers of
abstraction that will never be exploited can complicate and slow
down the code for no benefit.  Ask yourself:
<OL type=1>
  <LI> how likely is this change I am trying to anticipate?
  <LI> how hard would it be to accommodate it later if I did not
       make provisions for it now?
</OL>
If a change is unlikely, or if it would not be very difficult
to accommodate in the future, there is little pay-off for doing
extra work now to enable something that might happen in the future.  
This is a cost/benefit expectancy decision.
One of the principles of agile development is not to get too 
far ahead of yourself, designing features that you may never need.
</BODY>
</HTML>
