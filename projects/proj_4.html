<html><head>

<title>Implementat & Test</title>
</head><body>
<center>
<a name="desc_4">
<h1>Project Phase 4<br>
</a>
Implementation and Testing Sprint
</h1>
</center>
<P>
The first three projects took us through all of the activities that precede
implementation.  In this final project you will actually implement the components
you designed in phase 3 ... and by now it should come as little surprise to you
that only part of your time will be spent coding, and (if you have done it right) very 
little of your time will be spent debugging.
</P>

<table align="center" border="1" cellpadding="5" cellspacing="0">
<tbody>
  <tr>
	<th>Part</th>
	<th>Assignment</th>
	<th>Value<sup>3</sup></th>
  </tr>

  <tr>
	<td>4A</td>
	<td> <A href=#desc_4.1>Final Code</a> (individual)</td>
	<td> 25 </td>
  </tr>
  <tr>
	<td>4A</td>
	<td> <A href=#desc_4.5>Test Suite</a> (individual)</td>
	<td> 25 </td>
  </tr>

  <tr>
	<td>4B</td>
	<td> <A href=#desc_4.2>Pair Programming Exercise and Report</a> (team) </td>
	<td> 10 </td>
  </tr>

  <tr>
	<td>4B</td>
	<td> <A href=#desc_4.3>Code Review Notes and Reports</a> (team)</td>
	<td> 10 </td>
  </tr>

  <tr>
	<td>4B</td>
	<td> <A href=#desc_4.4>Test Driven Development Report</a> (team)</td>
	<td> 10 </td>
  </tr>


  <tr>
	<td>4C</td>
	<td> <A href=#desc_4.6>Sprint Review/Demo</a> (team)</td>
	<td> 10 </td>
  </tr>

  <tr>
	<td>4D</td>
	<td> <A href=#desc_4pm>Post-Mortem Report</a> (team)</td>
	<td> 10 </td>
  </tr>
</tbody>
</table>
<UL>
<sup>3</sup> less 10% for each <a href=#latepolicy>unexcused</a> late day.<br>
	<P>
	Each team member will use their code in one of the exercises
	(Pair Programming, Test Driven Development, or Code Review),
	and all team members will participate (as a submitter or reviewer)
	in at least one code review.  While most of these activities will involve selected
	individuals, you are encouraged to discuss each of these activities as a team,
	and the reports from most of those activities will be graded as team deliverables.
	<P>
	You may note that there is no management plan or grade associated with this
	project.  You should, by now be able to plan and coordinate activities for
	yourselves, and you are already being graded on your ability to deliver the 
	required work on schedule ... which is the point.
	</P>
</UL>

<a name="desc_4.1">
<H3>P4A.1 Final Code</h3>
</a>
<P>
The primary activity in this project is for each person to implement
and test the component they designed in project 3.  There will be
many processes and exercises surrounding this implementation, but
the primary deliverable is working code that implements the requirements
and specifications set out in project 3.
</P>
<P>
The primary deliverable is source files and scripts (e.g. ant/Makefile) required to build them.
Make sure that you document the build procedure and the environment that is required to build your
components (e.g. in a <tt>README.md</tt>), because part of your grade will depend on 
the grader being able to independently build your product from the checked-in sources 
and instructions.
If this is not practical (e.g. because your component cannot be built on a basic
Linux developer desktop) make arrangements with the professor (or Grutor) to have him/her 
do a check-out and build-from-scratch on an appropriate system.
</P>
<P>
You should also re-submit URLs for the specifications and design for this component
(from project 3) with any changes you have made since then.  These are the standard for
the completeness and correctness of your implementation.
</P>
<P>
After you have completed all of your implementation and testing, and
you believe your code is entirely complete
<ul>
    <li> prepare your source code (ideally ASCII text in file(s) with a language appropriate name)</li>
    <li> include a description of the build instructions and required environment in
         a <tt>README.md</tt> file.</li>
    <li> put a <a href="#submission">standard submission prologue</a> into a new file
         (<tt>code_4a.txt</tt>) which includes:
	 <ul>
	    <li> URL for the final component specifications</li>
	    <li> URL for the final component design</li>
	    <li> URL for the source file(s) in your repo ...  so we can examine their history</li>
	 </ul>
    </li>
    <li> up-load them to your Sakai Dropbox (each person needs to do this for themselves)</li>
</ul>
</P>
<P>
Each code submission will be graded on the basis of:
<ul>
	<li> 20% completeness with respect final specifications</li>
	<li> 20% being implemented as described in the final design</li>
	<li> 20% code quality (reasonable use of language/toolkit features, simplicity, efficiencty)
	<li> 20% readability (module and method comments, algorithmic comments, use of white-space)
	<li> 10% organization, build environment and instructions described in <tt>README.md</tt></li>
	<li> 10% grader is able to do a successful clone and build from scratch</li>
</ul>

<a name="desc_4.5">
<H3>P4A.2 Final Test Suite Results</h3>
</a>
<P>
Each team member will, for his/her component, implement the 
test plan proposed in project 3, and run (and pass) those tests against
their component implementation.
</P>
<P>
The execution of your test cases should be automated (e.g. so that
all tests can be run with a single command), and all of the test
cases and scripts should be checked in to git-hub.  
Make sure that you document the environment and procedure for running
these tests, because part of your grade will depend on the grader being able to
independently test your product from the checked-in sources and instructions
(in a <tt>README.md</tt> file).
If this is not practical (e.g. because your component cannot be tested on a 
basic Linux developer desktop)
make special arrangements with the professor (or Grutor) to have 
him/her do a check-out and build-and-test-from-scratch on an appropriate system.
</P>
<P>
It is possible that you will, as a result of lessons learned during 
the implementation, decide you want to change your test plan.  If this
happens
<OL type="a">
    <li>update your project 3 test plan accordingly </li>
    <li>include, with your submission, a summary and explanation of the changes</li>
</ol>
</P>
<P>
After you have completed all of your implementation and testing, and
you believe your test suite to be entirely complete
<ul>
    <li> prepare your source code (ideally ASCII text in file(s) with a language appropriate name)</li>
    <li> include a description of the required build and execution environment and the 
         build and run instructions in a <tt>README.md</tt> file.</li>
    <li> include a file (<tt>output_4a.txt</tt>) containing the captured output from 
    	 a successful run of your test suite</li>
    <li> put a <a href="#submission">standard submission prologue</a> into a new file
         (<tt>test_4a.txt</tt>) which includes:
	 <ul>
	    <li> URL for the final component specifications and requirements</li>
	    <li> URL for the final test plan</li>
	    <li> URL for the source file(s) in your repo ...  so we can examine their history</li>
	 </ul>
    </li>
    <li> up-load them to your Sakai Dropbox (each person needs to do this for themselves)</li>
</ul>
</P>
This activity and report be graded on the basis of:
<ul>
	<li> 20% completeness with respect to the final test plan</li>
	<li> 15% clear indications of what assertions are being tested, 
	         and the pass/fail of each test-case and the entire suite.</li>
	<li> 5% the checked-out code passes every test.</li>
	<li> 20% code quality (reasonable use of language/toolkit features, 
		reasonable setup/cleanup, simplicity, efficiency)
	<li> 15% readability (module and method comments, algorithmic comments, use of white-space)
	<li>  5% clear comments, in every test-case, about the assertion being tested
	  	 and the means of exercising it and ascertaining passage</li>
	<li> 10% grader is able to run and pass test suite</li>
	<li> 10% organization, build environment and instructions described in <tt>README.md</tt></li>
</ul>


<a name="desc_4.2">
<H3>P4B.1 Pair Programming Exercise</h3>
</a>
<P>
At least one member of the team will ask another member to join
them for at least one pair-programming session.  How you divide
up your effort (think/code, code/review, code/test) is entirely
up to you, and you are welcome to try various approaches.
</P>
<P>
For all work done in a pair-programming mode, the commit comments
made during those sessions should reflect the division of 
responsibilities under which that work was done.
</P>
<P>
NOTE:
<u>What ever component this is done for should not also be used for code review or TDD.</u>
</P>
<P>
After the end of each pair-programming session, each of the
people involved should jot down notes on what happened.  After
the component has been completed, the two people should get
together (ideally discussing it with the entre team) and 
write up a report on the experience.  This report should cover:
<UL>
   <li>	what role divisions you tried and how you decided how to organize those activities.</li>
   <li> how effectively each persons time was used (with each division of roles).</li>
   <li> the speed of code development, vs working alone.</li>
   <li> the quality of the code, vs working alone.</li>
   <li> pleasant or unpleasant aspects of the experience.</li>
   <li> how you would do this differently next time.</li>
</UL>
</P>
<P>
When you are ready to submit this report for grading
<ul>
    <li> prepare your report (ideally in an ASCII text file named <tt>pair_report.txt</tt>)</li>
    <li> put a <a href="#submission">standard submission prologue</a> onto the front of it
    	 which includes:
	 <ul>
	    <li> URLs for the file(s) created during the pair programming exercise.</li>
	    <li> references to the commits done during the pair programming sessions
	         (or just say "all")</li>
     	 </ul>
    </li>
    <li> up-load them to your Sakai Dropbox (only one person in the team needs to do this)</li>
</ul>
</P>

<P>
This activity and report be graded on the basis of:
<ul>
	<li> 20% documentation of collaboration in commit comments</li>
	<li> 20% reasonable role divisions, reasonably carried out</li>
	<li> 20% amount of work accomplished in this mode</li>
	<li> 20% quality of work accomplished in this mode</li>
	<li> 20% reasonable insights into the process</li>
</ul>

<a name="desc_4.3">
<H3>P4B.2 Code Review</h3>
</a>
<P>
At least one member of the team will write all of his/her code,
and <strong>before</strong> running test suites against it, submit that code for
review by the other members of their team.  
The other team members
will study the code, prepare notes and conduct a code review,
producing a report with must-fix/should-fix/advice items.
The author will make the appropriate revisions, and then move on
to testing.  After the code is working, the author will discuss
the process with the rest of the team and then write
up a report on the process.
<UL>
	<P>
	The author will prepare a review package including references
	to the component requirements, specifications, and design (perhaps modified) 
	from project 3, as well as the implementing code.
	</P>
	<P>
	Each of the other team members will review that package and prepare
	written notes, which will be up-loaded to Sakai 
	(e.g. in ASCII text files named <tt>notes_4b.txt</tt>)
	<strong>well prior</strong> to the review.
	</P>
	<P>
	After the review, the author will prepare a formal report 
	(listing all important conclusions) and checked in to github.
	</P>
	<P>
	The author will address all of the raised issues, and then complete the
	testing.  The commit comments for all such changes should mention the
	review issue to which they respond.
	</P>
	<P>
	After completing the testing (and any other required changes) the author
	will add information about how the testing went and what (if any) additional
	problems were discovered.
	</P>
</UL>
<P>
NOTE:
<u>What ever component this is done for should not also be used for TDD or pair programming.</u>
</P>
<P>
The author's review report should include (in addition to the usual information):
<UL>
   <LI>	how valuable the input received from the code review process was, 
  	and what kinds of problems it turned up.</li>
   <li> what kinds of problems remainined in the code after the code
	review process, and why they weren't found.</li>
   <li> the relative merits of doing code review before or after testing.</li>
   <li> would knowing that you were going to have a code review have
     	caused you to make any changes to your test plan?</li>
   <LI> how would you do this differently next time?</li>
</UL>

<P>
When you are ready to submit this report for grading
<ul>
    <li> prepare your report (ideally in an ASCII text file named <tt>review_4b.txt</tt>)</li>
    <li> put a <a href="#submission">standard submission prologue</a> onto the front of it
    	 which includes:
	 <ul>
	    <li> URLs for the reviewed file(s).</li>
     	 </ul>
    </li>
    <li> up-load it to your Sakai Dropbox (only one person in the team needs to do this)</li>
</ul>
</P>
<P>
The grading of the code review exercise will be based on:
<ul>
	<li> 10% notes: appropriate comments, reasonably organized, submitted before review</li>
	<li> 40% notes: thoroughness of study to which they attest</li>
	<li> 10% report: completness with respect to issues raised</li>
	<li> 10% report: clear disposition of every issue</li>
	<li> 20% report: code improvement value gained from the review process</li>
	<li> 10% report: insights gained regarding code reviews</li>
</ul>

<a name="desc_4.4">
<H3>P4B.3 Test Driven Development</h3>
</a>
<P>
At least one member of the team will use Test Driven Development to
implement his/her component, building and running the test cases for 
each increment of code as the new code is added.   The rewards for 
this approach should be:
<OL type="a">
   <LI> the there should be little debugging to do, and what little 
	there is should be quite simple.</li>
   <LI>	by the time the coding is done, most of the testing will also be done.</li>
</OL>
But it will require the test framework to be working first, and more up-front 
planning about the order in which things should be implemented.
Write-up and commit plan before you start coding.
There are a few tricks to this planning:
<UL>
   <LI> there is a natural order to implementation and testing, because
        some features fundamentally depend on others.  These dependencies
        must be recognized.</li>
   <LI> some test cases may be only applicable to incomplete code
	and become obsolete after the code has been completed.  These
	represent a waste of work and should be avoided as much as
	possible.</li>
</UL>
<P>
As evidence that you did infact follow a TDD process, and for keeping a
record of the problems found, please:
<UL>
   <LI> make sure that you commit the test cases together before
   	committing the code they test.
	(Note that we are not asking you to prove that you failed
	the test before you wrote the code).
	</li>
   <LI> after you have passed the tests, commit the updated code
        (and perhaps test cases), with comments describing
        the problems found and fixed during the testing.  If they
	worked first time (this will happen) follow this up with
	a trivial commit and a comment to that effect.</li>
   <LI> do not move on to implementing new functionality until
	you have passed all the tests for the existing functionality.</li>
</UL>
<P>
NOTE: 
<u>What ever component this is done for should not also be used for code review or pair programming.</u>
<P>
After completing development, the/each person who uses this methodology
will discuss the experience with the team and write a brief report, covering 
what they did, and specifically addressing the following questions:
<UL>
   <LI>	how the implementation/testing order was decided, and how the chosen order worked.</li>
   <LI> the efficiency of the process was (e.g. how much time went into building
 	test cases that were only useful during the construction process).</li>
   <LI> to what extent do you believe that knowing how you were going to
	test the code caused you to write code that was more correct?</li>
   <LI> were there bugs that showed up later that were not found by the
	TDD process?  If so, why do you think they were not turned up
	earlier?</li>
   <LI> how would you do this differently next time?</li>
</UL>

<P>
When you are ready to submit this report for grading
<ul>
    <li> prepare your report (ideally in an ASCII text file named <tt>tdd_4b.txt</tt>)</li>
    <li> put a <a href="#submission">standard submission prologue</a> onto the front of it
    	 which includes:
	 <ul>
	    <li> URLs for the files containing the code and test-cases.</li>
     	 </ul>
    </li>
    <li> up-load it to your Sakai Dropbox (only one person in the team needs to do this)</li>
</ul>
</P>
<P>
This activity and report be graded on the basis of:
<ul>
   <li>	10% a reasonable plan for implementation order (checked in before starting)</li>
   <li> 10% automation framework enabled testing from the start</li>

   <li> 20% tests were written and passed incrementally (from the commit history)</li>
   <li> 20% each test meaningfully validated the associated code (from the commit history)</li>
   <li> 20% tests were committed before the code they tested</li>
   <li>	20% reasonable insights gained from the process</li>
</ul>


<a name="desc_4.6">
<H3>P4.6 Sprint Review/Demo</h3>
</a>
<P>
You have built your components, and your test suite, and you have
run your tests.  Hopefully you have been able to combine your
components and demonstrate functionality for the integrated whole.
Now it is time for you to review what you have produced with your
product owner.
At the end of each SCRUM sprint, the team presents the work that was
completed during that sprint to the product owner.  This is, in part,
ceremonial (the team can claim success and receive feedback on the
work they have completed) but it also a very practical sign-off:
<UL>
    <LI> the team briefly reviews the requirements to have been
	 met and demonstrates that the product now meets them.</li>
    <LI> the team overviews the testing that has been done.</li>
    <LI> the product owner decides whether or not this work
         has actually been completed (i.e. deliverable
         functionality).</li>
    <LI> velocity points are earned for accepted work.</li>
</UL>
<P>
The test demonstration will go beyond the (isolated)
unit test case execution for each of the completed modules.
They should show evidence of correct integration and
interaction between those modules.  
Part of the score is based on that demonstration.
</P>

<P>
Your review presentation should include:
<UL>
   <LI>	a brief summary of the functionality of each of the components
	built during this sprint and the (component level) requirements
	it was to meet.</li>
   <LI>	a brief overview of the scope of the automated testing plan for
	each component.</li>
   <LI>	a brief overview of the expected functionality of the integrated
	pieces.</li>
   <LI>	a demonstration of a check-out from git-hub and (error-free) 
	build and automated test of each component (according to that test plan).</li>
   <LI>	a demonstration of the functionality of the combined pieces,
	showing them working together, and that all (applicable) key 
	requirements have been met.</li>
   <LI> a summary of what progress this represents towards the construction
	of your larger project (what this means and what comes next).</li>
</UL>
<P>
This is not a long presentation (4-5 minutes will be fine).  
It might be useful to have slides to cover the components, 
their requirements, and their test plans, but this is not
necessary. 
</P>
<P>
This presentation be graded on the basis of:
<ul>
	<li> 10% overview of components and their requirements</li>
	<li> 10% overview of component test plans</li>
	<li> 10% overview of integration and resulting functionality
	         of the combined sub-components.</li>
	<li> 20% error free check-out and build from scratch</li>
	<li> 20% error free check-out and execution of all test cases</li>
	<li> 20% demo shows component meets all of its key functional requirements</li>
	<li> 10% demo shows clear evidence of successful integration
	         (that all sub-components are clearly interoperating).</li>
</ul>
<P>
When you have an idea when you will be ready for your review, 
contact either the professor (or Grutor) to schedule it.

<a name="desc_4pm">
<H3>P4.7 Post-Mortem Report</H3>
</a>
<P>
This project is a learning exercise, and one of the major ways
we learn is by analyzing past mistakes.  You will, as a team,
review all aspects of this project.  One of you will then 
summarize that process into a post-mortem analysis report.
</P>
<P>
A report, summarizing the key issues raised in your post-mortem,
and the conclusions you came to.  Your post-mortem discussion 
should include:
<ul>
	<li> the implementation of your component design and the resulting code quality</li>
	<li> the implementation of your test plan, the problems found, and the confidence gained</li>
	<li> the relative efficacy of the competing development processes</li>
	<li> the integration of your components together</li>
	<li> the preparation of the release review/demo</li>
	<LI> the overall project as an educational exercise</li>
</ul>
</p>
<P>
This report be graded on the basis of:
<ul>
	<li> 50% whether or not you meaningfully discuss each of the required activities.</li>
	<li> 20% whether or not you identify all of the important incidents.</li>
	<li> 30% the extent to which you are able to derive and articulate useful lessons
	     (and good future advice) from those experiences.</li>
</ul>
<P>
Upload the notes from your post-mortem discussion to github, and fill out a
<a href="#submission">standard submission form</a>
and e-mail it to the submission alias.
</body></html>
