<html><head>
<title>Specs & Design</title>
</head><body>
<center>
<a name="desc_3">
<h1>Project Phase 3<br>
</a>
Specifications, Design and Review
</h1>
</center>

<P>
<H2>Introduction</h2>
<P>
The architecture has described the roles, functionality, and interfaces of each of
the key components in our system.  A few components may be simple enough
that we can simply sit down, code them up, and watch them work ... but most
components are more complex than that.  Before we start coding a non-trivial
component we need to make sure that:
<ul>
   <li> we understand (completely and in detail) what our component will do.</li>
   <li> we understand how we will implement those things.</li>
   <li> we understand how we will test the correctness of that implementation.</li>
   <li> we have not made any obvious mistakes in our plans for the above.</li>
</ul>
In this project, we will complete our implementation prerequisites.
</P>
<P>
The first two projects were largely team activities.  In the next two projects
you will do much of your work as individuals, tho with considerable assistance
from other team members.  Each member of the team will (over the next two projects)
be responsible for the design and implementation of one piece from your architecture.
Depending on your architecture, these could each be a complete architectural component,
or small pieces (e.g. an applet or a few classes) from a single architectural component.  
Choose your pieces carefully:
<UL>
   <li> each of the chosen pieces should be somewhere in the range of 
        100-400 lines of code.</li>
   <li> each chosen piece must implement an <em>algorithmically interesting</em>
        and automatically testable program, module, or class;
        putting up a web page or widgets on a screen does not count.</li>
   <li> each person must be capable of <u>independently</u> building and 
        testing his/her own piece.</li>
   <li> each of these pieces must be <u>automatically</u> unit-testable in isolation.</li>
   <li> when you are done, it <u>should</u> be possible to combine all
	of these individual pieces together to create a working 
	(all the pieces working together) and demo-able component.</li>
</UL>
Note, however, that you are each implementing only one (or a few) module(s) ...
just enough to require some design and coding work, and a modest number
of test cases.  It is not expected that the sum of these modules will
add up to your proposed product.
Neither must each of you implement a single complete architectural component:
<ul>
    <li> if a single component in your architecture is likely to be
    	 on the order of 400-800 lines of code, you can break it
	 up into multiple classes, and implement only a single class.</li>
    <li> if a single component of your architecture is trivial
         (e.g. 10 lines of code to do a database lookup in response
	 to a remote request), you might choose to implement
	 all of those functions as well as the client-side methods
	 that make those requests ... so that you were implementing
	 and testing the entirety of client/server/database interactions,
	 including the mechanisms that interconnect them.</li>
</ul>
</P>
<P>
Your pieces can be implemented in any appropriate language or combination of languages, 
and use any tool-kits or middle-ware you find convenient ... but it must be 
compilable/executable code with some algorithmic complexity (not U/I widgets, data, HTML or images),
and must be accompanied by a fully automated unit test suite.
</P>
<P>
The warning about choosing U/I components is based on two concerns:
<ol type="1">
   <li> Putting up U/I widgets and responding to their call-backs is
	so simple that it is commonly assigned as a project in introductory
	programming classes.</li>
   <li> If the primary inputs to your component are touches/clicks,
        and your primary outputs are pixels on the screen (a) it may be 
	very difficult to create an automated unit-test suite and (b)
	most of the code being tested is not yours, but the GUI tool kit.</li>
</ol>
But this does not mean that it is impossible for U/I components to satisfy the
design and implementation requirements:
<ul>
   <li> If your U/I component does significant analysis of the input
        (e.g. parsing, keyword recognition, and turning natural language
	input into a well-formed request), such processing completely
	qualifies as both non-trivial and auto-testable.</li>
   <li> The same might be the case if you have to do signficant processing
        to filter and transform complex data into a form that was useful
	to the intended user.</li>
   <li> Some U/I toolkits incorporate unit-testing frameworks, that
        permit user selections to be simulatable (e.g. <tt>click_button("Select File")</tt>)
	and results (to be displayed) tested by examining object state
	(e.g. <tt>chosen_file.getText()</tt>).  If your U/I is rich
	enough in functionality (e.g. several hundered lines of code
	involving complex widgets and event processing), and thoroughly 
	exercisable with such tools, that U/I might qualify as a component
	for design and implementation in the next phases of the project.</li>
	<P>
	If you believe you have a U/I component that would qualify, review it with
	me before completing your selections and plans.
	</P>
</ul>
</P>
<P>
In this project you will create specifications, designs and testing plans
for the chosen pieces.
In the next (and final) project you will execute these plans, building, 
testing, integrating, and demonstrating working software.
I suggest that you review the work you will have to do in projects 3 and 4,
and then give considerable thought to which components (or parts of which
components) you want to choose.
</P>
<P>
The complete specifications for the chosen components are due in the second week
of this project ... but you would be well advised to start defining the interfaces
that you will be exporting for use by other team members this week.
These interfaces represent critical inter-dependencies between otherwise individual
development efforts, and must be negotiated between the producers and consumers.
Sketching out those interfaces up-front will make it much easier to pursue your
(individual component) specification and design activities next week.
</P>
<P>
There are multiple phases to this project, each of which has its
own goals, processes, and deliverables (most of which are individual rather than team):
<table align="center" border="1" cellpadding="5" cellspacing="0">

<tbody>
  <tr>
	<th>Phase</th>
	<th>Assignment</th>
	<th>Value<sup>3</sup></th>
  </tr>

  <tr>
	<td> 3A </td>
	<td> <A href=#desc_3a>Plan</a> (for your component)</td>
	<td> 5 </td>
  </tr>

  <tr>
	<td> 3B </td>
	<td> <A href=#desc_3b>Component Specifications</a> (for your component)</td>
	<td> 10 </td>
  </tr>
  <tr>
	<td rowspan="2"> 3C </td>
	<td> <A href=#desc_3c1>Component Design</a> (for your component)</td>
	<td> 20 </td>
  </tr>
  <tr>
	<td> <A href=#desc_3c2>Component Test Plan</a> (for your component)</td>
	<td> 20 </td>
  </tr>

  <tr>
	<td rowspan="2"> 3D </td> 
	<td> <A href=#desc_3d1>Review Notes</a> (you prepared for other reviews) </td>
	<td> 10 </td>
  </tr>

  <tr>
	<td> <A href=#desc_3d2>Review Report</a> (from review of your component)</td>
	<td> 5 </td>
  </tr>

  <tr>
	<td rowspan="2"> 3E </td>
	<td> <A href=#desc_3e>Final Specifications, Design and Test Plan</a> (for your component)</td>
	<td> 15 </td>
  </tr>

  <tr>
	<td> <A href=#desc_3pm>Post-Mortem Report</a> (team)</td>
	<td> 10 </td>
  </tr>

  <tr>
	<td rowspan="2"></td>
	<td> <A href=#management>management</a> </td>
	<td> 5 </td>
  </tr>
</tbody>
</table>
<UL>
<sup>3</sup> less 10% for each <a href=#latepolicy>unexcused</a> late day.<br>
</UL>
<a name="desc_3a">
<H2>P3A Plan</h2>
</a>
<P>
The amount of work required to refine your architecture to the point that
it is possible to identify and specify your chosen components will vary
greatly from one team/product to the next, and I would encourage you to get
this behind you as quickly as possible.  Once you have a sense of what
the chosen components are, you should have a pretty good idea of how much
work it will be to do the designs and test plans.  You should, however,
leave yourself ample time for discovering issues in the review process
and making the required changes.
</P>
<P>
Perhaps the most important part of your plan is which components or classes
each of you will implement.
But each team will prepare a task-breakdown, identify the dependency relationships between
tasks (and components), and owners for each sub-task, assign due-dates, and schedule regular reviews
of both work-products and progress (to enable adequate time to deal with the
<u>problems that will arise</u>).  
A good management plan will include regular (e.g. daily) status checks, whose
results should be recorded in a <tt>minutes.txt</tt> file in your repo.
</P>
<P>
As your understanding of the problem
evolves and you respond to unanticipated events, 
you will have to revise your plan (not merely estimates, but
the work to be done).  Make sure that you document each of these
problems and the manner in which you decide to respond to it.
If deadlines are missed, or deliverables fail to pass review, the fact, as well
as the causes and the plan to remedy them must be documented.
</P>
<P>
Your initial Management Plan will be graded on the basis of:
<ul>
	<li> 20% well chosen components (in terms of size, complexity, testability)</li>
	<li> 20% good use of time and resources (work spread reasonably over the available time)</li>
	<li> 20% specificity of plan (clear responsibilities: what, when)</li>
	<li> 20% provisions for early detection of problems, and time to deal with them</li>
	<li> 20% completeness</li>
</ul>
<P>
Maintain your plan (and status update minutes) on github.
You will probably be updating them daily,
and we will be reviewing this history.
</P>
<P>
When you are ready to submit your plan for grading:
<ul>
    <li> prepare your management plan (<tt>management_3a.???)</tt></li>
    <li> put a <a href="#submission">standard submission prologue</a> on the front of it</li>
    <li> up-load it for submission (each person must submit his/her own plan)</li>
</ul>
</P>

<a name="desc_3b">
<H2>P3B Specifications</h2>
</a>
<P>
Each team member will take ownership of one or more modules (or components).
To prepare a specifications, you will:
<OL type="1">
  <li> (perhaps) expand the architecture (above the chosen components)
        to describe them and the components with which they interact.
	If you have decided to implement pieces of your system that
	are smaller than complete architectural components (from project 2), 
	you will have to expand and refine that architecture down to the 
	level of the pieces you want to use for projects 3 and 4.  Note that
	you do not have to expand everything in your architecture to this
	level of detail.  You only have to do a top-down refinement along
	the path to the components you will be using for this project.
	<p>
	If a component to be implemented was already <u>fully described</u>
	(detailed specifications for all external interfaces)
	in the (Project 2) architecture, no further expansion is required.
	If further top-down refinement is required, your component specifications
	should be accompanied by addenda to the (submitted for project 2) architecture.
	It may be possible that a single addendum (created by the entire team)
	could be used for all of the component specifications.
    	</p>
	</li>
  <li> generally describe the functionality of each of the component(s)
	to be designed, and their role(s) in the overall architecture.</li>
  <li> determine the requirements to be imposed on each of the component(s) to
      	be designed, based on the product requirements 
	and the components' roles in the overall architecture.</li>
  <li> define the all external interfaces (in both directions) between the 
  	component(s) to be designed and the rest of the system.</li>
  <li> write a <u>complete specification</u> (both form and function,
       detailed enough to define acceptance criteria)
       for all of the external interfaces to the chosen components.
       </li>
</OL>
<P>
This submission be graded on the basis of:
<ul>
	<li> 30% clarity and reasonableness of component's functionality and role in the overall system,
	         and its suitability (in terms of size, complexity, testability) for the assignment.</li>
	<li> 20% clarity and measurability of requirements and how well they have been derived from
	     the architecture and overall product requirements</li>
	<li> 10% clear and complete (ready-to-code) interface specifications</li>
	<li> 10% spec covers all required functionality in this module</li>
	<li> 20% well abstracted module functionality and reasonably chosen methods and parameters</li>
	<li> 10% interface definitions lend themselves to automated compliance testing</li>
</ul>
<P>
When you are ready to submit your component specifications for grading:
<ul>
    <li> prepare your spec (ideally ASCII text in a file named <tt>spec_3b.txt</tt>)</li>
    <li> if you need to expand your (project 2) architectural description, create an
         addendum (only the changes, not a whole new copy, in a file named <tt>arch_3b.txt)</tt></li>
    <li> put a <a href="#submission">standard submission prologue</a> on the front of each file</li>
    <li> up-load it/them for submission (each person must submit his/her own specifications)</li>
</ul>
</P>
<a name="desc_3c1">
<H2>Design and Test Plan</H2>
<H3>P3C.1 Component Design</h3>
</a>
<P>
Each team member will prepare a detailed design for one or 
more modules, ideally comprising 100-400 lines of code when complete.  
This design need not be at the level of complete pseudo-code, but 
(in combination with the specifications) should be sufficient to 
enable a skilled programmer to easily and correctly implement the specified module(s).
</P>
<P>
Each module design should include:
<UL>
   <LI> a description of the purpose of this module.</li>
   <LI> complete declarations for all public methods and instance variables.</li>
   <LI> complete declarations for all private methods and instance variables.</li>
   <LI> descriptions of purposes for all methods/routines and meanings of all parameters.</li>
   <LI> descriptions and uses of all non-trivial instance variables and data structures.</li>
   <LI> general descriptions of all non-obvious algorithms.
	<br>
	These need not be pseudo code, but could simply be 
	descriptions of an approach (e.g. "allocate from the heap",
	"bubble-sort" or "compute SHA1 hash").  
	If a perhaps-not-widely-known named
	algorithm is used, provide a reference to a description
	of its implementation (in case the programmer who will 
	reivew or implement it isn't familiar with it).</li>
   <LI> where the code is to be based on standard libraries,
	call those out (e.g. "synchronization will be with
	boost thread shared locks") and (again, if they are not widely known)
	include a reference to the documentation for the functions to be used.</li>
</UL>
</P>
<P>
If any of these design elements are non-obvious, the rationale 
for those decisions should be described so that the implementer
can better understand what must be done.  People are more likely
to make mistakes when working on things they do not understand.
</P>
<P>
You can prepare your designs in any form you find convenient,
but you may find it easiest to create them as
code modules with compilable declarations, extensive
(e.g. JavaDoc/PyDoc) comments and very little actual code.  
Overview and rationale can be presented as comments in front of the described elements.
Many people choose to start non-trivial implementations by writing the
declarations and comments.
</P>
<P>
If the chosen component is of reasonable size, complexity, and testability
this design be graded on the basis of:
<ul>
	<li> 10% all external methods and instance variables are 
		described well enough to enable a skilled client
		figure out how to use them correctly.</li>
	<li> 20% all implementations and internal methods are 
		described well enough to enable a skilled programmer
		to implement them (with no more research than having
		to read the referenced documentation).</li>
	<li> 10% all variables and data structures are described
		well enough to enable a skilled programmer to use
		and update them correctly.</li>
	<li> 10% the defined methods and instance variables are
		sufficient to fully implement the component interface 
		specification.</li>

	<li> 10% the reasonableness (after having read the rationale)
		of the specified implementations.</li>
	<li> 10% the correctness of the described implementations.</li>
	<li> 10% reasonable exploitation of language and tool capabilities.</li>
	<li>  5% the simplicity (relative to the problem) of the proposed implementations.</li>
	<li>  5% the likely efficiency of the described implementations.</li>

	<li> 10% the readability of the design document, including the
	         clarity and adequacy of the overview and rationale.</li>
</ul>

<P>
When you are ready to submit your component design for grading:
<ul>
    <li> prepare your spec (ideally ASCII text in a file named <tt>design_3b.txt</tt>,
         or perhaps with some other language-specifc suffix, e.g. if you are using graphics)</li>
    <li> put a <a href="#submission">standard submission prologue</a> on the front of the file</li>
    <li> up-load it for submission (each person must submit his/her own design)</li>
</ul>
</P>

<a name="desc_3c2">
<H3>P3C.2 Component Test Plan</h3>
</a>
<P>
Once you have specifications and a design to satisfy them, you have
to figure out how you will test your implementation, to demonstrate
that it actually works.
<UL>
   <LI>
	Review your specifications, and develop a set of automated comprehensive
	(black box) test cases to determine whether or not your component meets 
	its interface requirements and specifications.</li> <LI>
	Review your design, and consider any possible/plausible component
	misbehavior that would not be detected by the the above-described
	acceptance tests.  If you find such, define additional
	(white box) test cases to exercise and validate those behaviors.
	If there is no plausible mis-behavior not covered by the interface
	acceptance criteria, 
	<u>you can earn the white-box points by offering a convincing proof of this assertion</u>.</li>
   <LI>
	Consider your component's interfaces and all of the above test
	cases, and design a means for performing all of those test cases
	in a fully automated fashion ... ideally using an existing
	unit testing framework (to structure your test case implementations,
	invoke them, and collect the results).</li>

   <LI>
	Review the overall list for completeness (is everything covered),
	efficiency (are there redundant tests) and value (are there
	tests for things that don't matter or will never happen), and
	try to optimize out the low-value tests.  If there are no such
	tests, briefly describe the analysis that led you to that conclusion.</li>
</UL>
<P>
It is possible that, in the process of developing your test plan, you will find
assertions that are hard to measure, or functionality that is difficult
to verify (or verify automatically).  If this happens, you may need to
revisit your requirements, specifications and design.
</P>

<P>
Your test plan should include:
<UL>
   <LI>	
	A general overview of the functionality to be verified,
	and the general approach(es) that will be taken to verification.</LI>
   <LI>
	An index of test cases.</LI>
   <LI>
	A description of the framework under which this testing will be performed.</LI>
   <LI>
	A complete list of test cases ... for each:
	<UL>
	    <LI> a brief summary of the assertion to be tested</LI>
	    <LI> the justification (e.g. traceability to specifications or requirements) for including this test case</LI>
	    <LI> any special set-up required to run this test</LI>
	    <LI> how the situation to be tested will be generated</LI>
	    <LI> how correctness of behavior will be ascertained</LI>
	    <LI> any special clean-up required after this test</LI>
	</UL>
    </LI>
</UL>

<P>
Your test plan will be graded on the basis of:
<ul>
	<li> 30% black box tests
	<UL>
	    <li> traceability to specifications</li>
	    <li> reasonableness of test cases</li>
	    <li> completeness of suite</li>
	    <li> form and quality of descriptions</li>
	</UL>
	</LI>
	<li> 30% white box tests
	<UL>
	    <li> traceability to design</li>
	    <li> reasonableness of test cases</li>
	    <li> completeness of suite</li>
	    <li> form and quality of descriptions</li>
	</UL>
	</LI>
	<li> 20% testing framework
	<UL>
	    <li> automated execution and pass/fail determination </li>
	    <li> use of off-the-shelf technology</li>
	    <li> reasonablenss practicality of using this tool for this testing</li>
	</UL>
	</LI>
	<li> 10% overall completeness (how much confidence this suite will give us)</li>
	<li> 10% overall efficiency (low value or redundant tests, overly expensive executions)</li>
</ul>

<P>
When you are ready to submit your test plan for grading:
<ul>
    <li> prepare your spec (ideally ASCII text in a file named <tt>test_3b.txt</tt>)</li>
    <li> put a <a href="#submission">standard submission prologue</a> on the front of the file</li>
    <li> up-load it for submission (each person must submit his/her own design)</li>
</ul>
</P>

<a name="desc_3d">
<H2>Design and Test Plan Reviews</H2>
</a>
<P>
Because your component has some algorithmic complexity and requires a non-trivial number
of test cases your design and test plans should be submitted for review.
In Project 2 we required you to follow a fairly formal process (with another team
supplying the facilitator and scribe).  This is a simpler design, entirely appropriate
to be reviewed by other members of your team (all of whom should already be well-familiar
with what your component does).
You are welcome, for this less formal process, to act as the facilitator and scribe 
for your the review of your own component.
But the other basic rules (e.g. about content, scope and behavior) still apply:
<ul>
   <li> the committee should still vote on the resolution (must fix, should fix, comment)
        of each issue/observation.</li>
   <li> a report, covering all non-trivial issues and their resolutions must be
        produced <u>and approved by the reviewing team</u>.
</ul>
</P>
<P>
Each member of the team will submit his/her preliminary specifications,
design and test plans for review by the one or more other team members.  Each
team member will participate in the reviews of the designs and plans
submitted by other members of his/her team.
</P>
<a name="desc_3d1">
<H3>P3D.1 Review Notes</h3>
</a>
<P>
Prior to each review meeting, each of you (individually) will read
the submitted specifications, designs, and test plan and prepare detailed notes
on all questions and concerns.  These notes must be submitted at
least 24 hours prior to the actual review session.  They should be
neat notes, describing legitimate issues clear enough to be sent
as email, and organized for discussion (e.g. in a reasonable 
review order).
</P>

<P>
Each set of review notes will be graded on the basis of:
<ul>
	<li> 40% submitted 24 hours before scheduled review</li>
	<li> 30% the thoroughness of study to which they attest</li>
	<li> 10% how well articulated the issues are</li>
	<li> 10% all comments appropriate and within scope</li>
	<li> 10% issues reasonably organized for discussion</li>
</ul>

<P>
When your notes are eady for submission and grading:
<ul>
    <li> prepare your spec (ideally ASCII text in a file named <tt>notes_3d-#.txt</tt>,
         where <tt>#</tt> is different for each review).</li>
    <li> put a <a href="#submission">standard submission prologue</a> on the front of the file</li>
    <li> up-load it for submission (each person must submit his/her own notes)</li>
</ul>
</P>
<a name="desc_3d2">
<H3>P3D.2 Review and Report</h3>
</a>
<P>
You will conduct  design reviews for each submitted
Specification/Design/Test Plan package.  The process will similar
to the architectural review ... but because this is 
simpler and you have already followed this process 
(for your architectural reviews) these
reviews will not be observed and graded.  
But each team member will lead, act as scribe, and write up
a report for one design review (most likely their own).
</P>
<P>
As with the architectural review,
this is not "meeting minutes".  Rather it is a distillation of key issues and
decisions.  It must contain:
<UL>
   <LI> the time, date, and attendees</li>
   <LI> the component to be reviewed</li>
   <LI> a clear summary of each important issue rased</li>
   <LI> a characterization as a defect, question, or issue</li>
   <LI> a characterization as must-fix, should-fix, or comment</li>
   <LI> a disposition for the entire proposal of
   <UL>
	<li> approved</li>
	<li> approved with required changes</li>
	<li> requires another meeting</li>
	<li> rejected (this cannot be made to work)</li>
   </UL>
   </LI>
</UL>
</P>
<P>
Each review report will be graded on the basis of:
<ul>
	<li> 10% form: time, place, project, attendees</li>
	<li> 20% scope of the report (just issues, no opinions)</li>

	<li> 50% clarity with which issues are presented</li>
	<li> 10% clear disposition for each issue</li>
	<li> 10% disposition of the entire project</li>
</ul>

<P>
When your review report is eady for submission and grading:
<ul>
    <li> prepare your spec (ideally ASCII text in a file named <tt>review_3d.txt</tt>)</li>
    <li> put a <a href="#submission">standard submission prologue</a> on the front of the file</li>
    <li> up-load it for submission (each person must submit his/her own notes)</li>
</ul>
</P>

<H2>P3E Final Specifications, Design, Test Plan  and Post Mortem</H2>
<P>
Note: this is likely to be a relatively light week for project work.
      You would be well advised to use this opportunity to get an
      early start on the project 4 implementation, which probably 
      comes due a the same time as the final projects in your other classes.
</P>
<a name="desc_3e">
<H3>P3E.1 Final Component Specifications, Design and Test Plan</h3>
</a>
<P>
It is likely that your design and test case development, and their
reviews will turn up issues that require changes to your specifications, 
design and test plan.  
Address those issues (by revising your specifications, design and test plan), 
document the changes that were made to address each, and get agreement from the 
reviewers that the issues have been satisfactorally addressed.
</P>
<P>
Your final specification should contain:
<OL type=1>
   <LI> discussion of issues that were discovered and
        summary of changes made and reasons for each.</LI>
   <LI> descriptions of changes to the (project 2) architecture.</LI>
   <LI> updated component specification.</LI>
   <LI> updated updated component design.</LI>
   <LI> updated test plan.</LI>
</OL>
</P>
<P>
This submission be graded on the basis of:
<ul>
	<li> 50% discussions of and responses to issues that arose</li>
	<li> 50% quality of the revised specifications, design and test plan
	         (using the same criteria as phases B and C)
	</li>
</ul>
</P>
<P>
When you have addressed all of the issues raised in your review and are
ready to submit your final component specifications, design and plan,
<ul>
    <li> prepare your spec (ideally ASCII text in a file named <tt>spec_3e.txt</tt>)</li>
    <li> prepare your design (ideally ASCII text in a file named <tt>design_3e.txt</tt>,
         or perhaps with some other language-specifc suffix)</li>
    <li> prepare your test plan (ideally ASCII text in a file named <tt>test_3e.txt</tt>)</li>
    <li> put a <a href="#submission">standard submission prologue</a> on the front of each file</li>
    <li> up-load it for submission (each person must submit his/her own files)</li>
</ul>
</P>

<a name="desc_3pm">
<H3>P3E.2 Post-Mortem Report</H3>
</a>
<P>
This project is a learning exercise, and one of the major ways
we learn is by analyzing past mistakes.  You will, as a team,
review all aspects of this project.  One of you will then 
summarize that process into a post-mortem analysis report.
</P>
<P>
A report, summarizing the key issues raised in your post-mortem,
and the conclusions you came to.  Your post-mortem discussion 
should include:
<ul>
	<li> architectural refinement and specification development.</li>
	<li> development of the component designs.</li>
	<li> development of the test plans.</li>
	<li> the review process and the resulting design/plan changes.</li>
	<li> the planning and ongoing management of these activities.</li>
	<li> the overall project as an educational exercise.</li>
</ul>
</P>
<P>
The submission and grading of Post Mortem reports is described
in the <a href="#postmortem">General Grading</a> information.
<P>
Make sure that you have kept your meeting minutes and management plan up-to-date on Github.
When you are ready to submit the Post-Mortem report (and management notes) for grading:
<ul>
    <li> prepare your report (ideally ASCII text in a file named <tt>postmortem_3.txt</tt>)</li>
    <li> put a <a href="#submission">standard submission prologue</a> on the front of it</li>
    <li> make sure that it also contains Github URLs for your status updates and management plans</li>
    <li> up-load it for submission (only one person on the team needs to do this)</li>
</ul>
</P>
<P>
This report be graded on the basis of:
<ul>
	<li> 50% whether or not you meaningfully discuss each of the required activities.</li>
	<li> 20% whether or not you identify all of the important incidents.</li>
	<li> 30% the extent to which you are able to derive and articulate useful lessons
	     (and good future advice) from those experiences.</li>
</ul>
</body></html>
