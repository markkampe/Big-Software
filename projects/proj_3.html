<html><head>
<title>Specs & Design</title>
</head><body>
<center>
<a name="desc_3">
<h1>Project Phase 3<br>
</a>
Specifications, Design and Review
</h1>
</center>

<P>
<H2>Introduction</h2>
<P>
The architecture has described the roles, functionality, and interfaces of each of
the key components in our system.  A few components may be simple enough
that we can simply sit down, code them up, and watch them work ... but most
components are more complex than that.  Before we start coding a non-trivial
component we need to make sure that:
<ul>
   <li> we understand (completely and in detail) what our component will do.</li>
   <li> we understand how we will implement those things.</li>
   <li> we understand how we will test the correctness of that implementation.</li>
   <li> we have not made any obvious mistakes in our plans for the above.</li>
</ul>
In this project, we will complete our implementation prerequisites.
</P>
<P>
The first two projects were largely team activities.  In the next two projects
you will do much of your work as individuals, tho with considerable assistance
and collaboration from other team members.
In this project you will create specifications, designs and testing plans
for chosen components.
In the next (and final) project you will execute these plans, building, 
testing, integrating, and demonstrating working software.
I suggest that you review the work you will have to do in projects 3 and 4,
and then give considerable thought to which components (or parts of which
components) you want to choose.
Depending on your architecture, these could each be a complete architectural component,
or small pieces (e.g. an applet or a few classes) from a single architectural component,
or even from multiple components:
</P>
<UL>
   <li> each of the chosen pieces should be somewhere in the range of 
        100-400 lines of code.</li>
   <li> each chosen piece <u>must</u> implement an <em>algorithmically interesting</em>
	(much more than initializers, mutators, accessors, and routines that
	pass a request on to another service) program, module, or class;
	</li>
   <li> each chosen piece <u>must</u> have sufficiently complex behavior to require
   	a significant number of test cases ... including several
	white-box test cases (that go beyond simple verification vs interface
	specifications).</li>
   <li> each person should be capable of <u>independently</u> building and 
        testing his/her own piece.</li>
   <li> each of these pieces <u>must</u> be <u>automatically</u> unit-testable.</li>
   <li> when you are done, it <u>should</u> be possible to combine all
	of these individual pieces together to create a working 
	(all the pieces working together) and demo-able component.</li>
</UL>
Note, however, that you are each implementing only one (or a few) module(s) ...
just enough to require a reasonable amount of design and coding work, and 
a modest number of test cases.  It is not required that the sum of these 
modules must add up to your proposed product.
Neither must each of you implement a single complete architectural component:
<ul>
    <li> if a single component in your architecture is likely to be
    	 on the order of 400-800 lines of code, you can break it
	 up into multiple classes, and implement only a single class.</li>
    <li> if you very much want to implement a component that is too simple
         (because of the value it adds to your program), you
    	 may need to choose multiple components to implement.  But if
	 none of those components meet the complexity and test case
	 requirements, you still may not be able to earn full credit
	 in projects 3 and 4.
	 </li>
</ul>
It is very important that you consider these requirements when choosing
the components to be implemented.  If you have any questions or doubts,
please talk to me or your grutor before finalizing your decisions.
</P>
<P>
Your pieces can be implemented in any appropriate language or combination of languages, 
and use any tool-kits or middle-ware you find convenient ... but it must be 
compilable/executable code with some algorithmic complexity (not U/I widgets, data, HTML or images),
and must be accompanied by a fully automated unit test suite.
</P>
<P>
The warning about choosing U/I components is based on two concerns:
<ol type="1">
   <li> Putting up U/I widgets and responding to their call-backs is
	so simple that it is commonly assigned as a project in introductory
	programming classes.</li>
   <li> If the primary inputs to your component are touches/clicks,
        and your primary outputs are pixels on the screen (a) it may be 
	very difficult to create an automated unit-test suite and (b)
	most of the code being tested is not yours, but the GUI tool kit.</li>
</ol>
But this does not mean that it is impossible for U/I components to satisfy the
design and implementation requirements:
<ul>
   <li> If your U/I component does significant analysis of the input
        (e.g. syntax parsing, keyword recognition, validity checking,
	carrying out complex request protocols)
	such processing might qualify as both non-trivial and auto-testable.</li>
   <li> The same might be the case if you have to do signficant processing
        (e.g. turn partial-order pairs into a visual tree) to filter and transform
	complex data into a form that is usefully presentable to the intended user.</li>
   <li> Some U/I toolkits incorporate unit-testing frameworks, that
        permit user selections to be simulatable (e.g. <tt>click_button("Select File")</tt>)
	and results (to be displayed) tested by examining object state
	(e.g. <tt>chosen_file.getText()</tt>).  If your U/I is rich
	enough in functionality (e.g. several hundered lines of code
	involving complex widgets and event processing), does much
	error checking, and and is thoroughly exercisable with such tools, 
	that U/I might qualify as a component for design and implementation
	in the next phases of the project.</li>
	<P>
	If you believe you have a U/I component that would qualify, review it with
	me before completing your selections and plans.
	</P>
</ul>
</P>
<P>
The complete specifications for the chosen components are due in the second week
of this project ... but you would be well advised to start defining the interfaces
that you will be exporting for use by other team members this week.
These interfaces represent critical inter-dependencies between otherwise individual
development efforts, and must be negotiated between the producers and consumers.
<ol type="a">
   <li>	Sketching out those interfaces up-front will make it much easier to pursue your
	(individual component) specification and design activities next week.</li>
   <li> As you pursue your own designs, you will likely come to a better understanding
        of what functionality (and interfaces) you need from the other components with
	which you interact.</li>
</ol>
Thus, component interfaces must be well specified before you begin your detailed designs,
but it is quite likely that those interfaces will change as a result of understandings
gained in the process of doing those detailed designs.
Even though detailed designs are not due until the next phase, you should already have
some pretty good ideas about how each component will be implemented.  If not, you may
have specified something that cannot be built ... and all of the work done based on
that specification may wind up being thrown away.
</P>
<P>
There are multiple phases to this project, each of which has its
own goals, processes, and deliverables (most of which are individual rather than team):

<IMG src="proj_3.jpg">

<table align="center" border="1" cellpadding="5" cellspacing="0">

<tbody>
  <tr>
	<th>Phase</th>
	<th>Assignment</th>
	<th>Value<sup>3</sup></th>
  </tr>

  <tr>
	<td rowspan="2"> 3A </td>
	<td> <A href=#desc_plan3>Plan</a> (for your component)</td>
	<td> 5 </td>
  </tr>

  <tr>
	<td> <A href=#desc_spec>Component Specifications</a> (for your component)</td>
	<td> 10 </td>
  </tr>
  <tr>
	<td rowspan="2"> 3B </td>
	<td> <A href=#desc_dsgn>Component Design</a> (for your component)</td>
	<td> 20 </td>
  </tr>
  <tr>
	<td> <A href=#desc_test>Component Test Plan</a> (for your component)</td>
	<td> 20 </td>
  </tr>

  <tr>
	<td rowspan="2"> 3C </td> 
	<td> <A href=#desc_notes3>Review Notes</a> (you prepared for other reviews) </td>
	<td> 10 </td>
  </tr>

  <tr>
	<td> <A href=#desc_report3>Review Report</a> (from review of your component)</td>
	<td> 5 </td>
  </tr>

  <tr>
	<td rowspan="2"> 3D </td>
	<td> <A href=#desc_final3>Final Specifications, Design and Test Plan</a> (for your component)</td>
	<td> 20 </td>
  </tr>

  <tr>
	<td> <A href=#desc_3pm>Post-Mortem Report</a> (team)</td>
	<td> 10 </td>
  </tr>
</tbody>
</table>
<UL>
<sup>3</sup> less 10% for each <a href=#latepolicy>unexcused</a> late day.<br>
</UL>
<a name="desc_plan3">
<H2>P3A.1 Plan</h2>
</a>
<P>
The amount of work required to refine your architecture to the point that
it is possible to identify and specify your chosen components will vary
greatly from one team/product to the next, and I would encourage you to get
this behind you as quickly as possible.  Once you have a sense of what
the chosen components are, you should have a pretty good idea of how much
work it will be to do the designs and test plans.  You should, however,
leave yourself ample time for discovering issues in the review process
and making the required changes.
</P>
<P>
Perhaps the most important part of your plan is which components or classes
each of you will implement.
But each team will prepare a task-breakdown, identify the dependency relationships between
tasks (and components), and owners for each sub-task, assign due-dates, and schedule regular reviews
of both work-products and progress (to enable adequate time to deal with the
<u>problems that will arise</u>).  
A good management plan will include regular (e.g. daily) status checks, whose
results should be recorded in a <tt>minutes.txt</tt> file in your repo.
</P>
<P>
As your understanding of the problem
evolves and you respond to unanticipated events, 
you will have to revise your plan (not merely estimates, but
the work to be done).  Make sure that you document each of these
problems and the manner in which you decide to respond to it.
If deadlines are missed, or deliverables fail to pass review, the fact, as well
as the causes and the plan to remedy them must be documented.
</P>
<P>
Your initial Management Plan will be graded on the basis of:
<ul>
	<li> 20% well chosen components (in terms of size, complexity, testability)</li>
	<li> 20% good use of time and resources (work spread reasonably over the available time)</li>
	<li> 20% specificity of plan (clear responsibilities: what, when)</li>
	<li> 20% provisions for early detection of problems, and time to deal with them</li>
	<li> 20% completeness</li>
</ul>
<P>
Maintain your plan (and status update minutes) with history (e.g. on Github or in Google Docs).
You will probably be updating them daily, and we will be reviewing this history.
</P>
<P>
When you are ready to submit your plan for grading:
<ul>
    <li> prepare your management plan (<tt>management_3a.???)</tt></li>
    <li> put a <a href="#submission">standard submission prologue</a> on the front of it</li>
    <li> up-load it for submission (each person must submit his/her own plan)</li>
</ul>
</P>

<a name="desc_spec">
<H2>P3A.2 Specifications</h2>
</a>
<P>
Each team member will take ownership of one or more modules (or components).
To prepare a specifications, you will:
<OL type="1">
  <li> (perhaps) expand the architecture (above the chosen components)
        to describe them and the components with which they interact.
	If you have decided to implement pieces of your system that
	are smaller than complete architectural components (from project 2), 
	you will have to expand and refine that architecture down to the 
	level of the pieces you want to use for projects 3 and 4.  Note that
	you do not have to expand everything in your architecture to this
	level of detail.  You only have to do a top-down refinement along
	the path to the components you will be using for this project.
	<p>
	If a component to be implemented was already <u>fully described</u>
	(detailed specifications for all external interfaces/public methods)
	in the (Project 2) architecture, no further expansion is required.
	If further top-down refinement is required, your component specifications
	should be accompanied by addenda to the (submitted for project 2) architecture.
	It may be possible that a single addendum (created by the entire team)
	could be used for all of the component specifications.
    	</p>
	</li>
  <li> generally describe the functionality of each of the component(s)
	to be designed, and their role(s) in the overall architecture.</li>
  <li> determine the requirements to be imposed on each of the component(s) to
      	be designed, based on the product requirements 
	and the components' roles in the overall architecture.</li>
  <li> define the all external interfaces/public methods (in both directions)
   	between the component(s) to be designed and the rest of the system.</li>
  <li> write a <u>complete specification</u> (both form and function,
       detailed enough to define acceptance criteria)
       for all of the external interfaces/public methods to the chosen components.
       </li>
</OL>
<P>
There is no universal definition for what "specifications" should describe,
in how much detail.  The answers to those questions depend on:
<ul>
   <li>what is being described</li>
   <li>to whom</li>
   <li>for what purpose</li>
</ul>
</P>
In this case, we are asking you to describe
<ul>
   <li> the external entrypoints (public methods) to be exposed by your component
   	<ul>
	    <li>the (exact) name of the method</li>
	    <li>the name, type, and meaning of each parameter,
	        and any validity assertions that might apply to it</li>
	    <li>everything (at the functionality level) that is expected to
	        happen as a result of having called this method</li>
	    <li>the type(s) and meaning(s) of the return value(s)</li>
	</ul>
   </li>
   <li> to your team members and your grader (who have a reasonable familiarity with your project
        and the toolkit(s) to be used in its construction) </li>
   <li> it is intended that these specifications be sufficiently detailed:
        <ul>
   	    <li>to enable other people to write correct code to call those entrypoints 
	         and use the services this component provides</li>
	    <li>to provide an adequate specification for developing a complete set
	        of black-box test cases</li>
	    <li>to completely specify all of the requirements that must be met by
	        the (coming soon) design and implementation.</li>
        </ul>
    </li>
</ul>
<P>
It is possible that your component will have no requirements beyond delivering above-described
functionality.  But it is likely that, as a result of its role in the overall architecture,
and the things that other components need it to do, there will be additional requirements
imposed on the manner in which your component provides its functionality or cases that must
be handled.
</P>
<P>
This submission be graded on the basis of:
<ul>
	<li> 30% clarity and reasonableness of component's functionality and role in the overall system,
	         and its suitability (in terms of size, complexity, testability) for the assignment.</li>
	<li> 20% completeness, clarity and measurability of requirements: 
	     <ul>
	         <li>derived from the architecture and overall product requirements</li>
		 <li>new requirements arising from the component definition</li>
	     </ul>
	     </li>
	<li> 10% spec covers all required functionality in this module</li>
	<li> 10% clear and complete (ready to compile) declarations for all public methods 
	     and instance variables</li>
	<li> 10% clear and complete (ready to code) functionality descriptions for all public methods</li>
	<li> 10% well abstracted module functionality, and reasonably chosen methods and parameters</li>
	<li> 10% interface definitions lend themselves to automated compliance testing</li>
</ul>
<P>
Maintain your specifications with history (e.g. on Github or in Google Docs).
When you are ready to submit your component specifications for grading:
<ul>
    <li> create a <a href="#submission">standard submission prologue</a> 
         (that includes your specifications or a URL to them), and
	 entitle it <tt>spec_3b.txt</tt></li>
    <li> if you need to expand your (project 2) architectural description, create an
         addendum (only the changes, not a whole new copy) and either include those
	 changes (or a URL to them) in front of your specifications.</li>
    <li> up-load it for submission (each person must submit his/her own specifications)</li>
</ul>
</P>
<a name="desc_dsgn">
<H2>Design and Test Plan</H2>
<H3>P3B.1 Component Design</h3>
</a>
<P>
Each team member will prepare a detailed design for one or 
more modules, ideally comprising 100-400 lines of code when complete.  
This design need not be at the level of complete pseudo-code, but 
(in combination with the specifications) should be sufficient to 
enable a skilled programmer to easily and correctly implement the specified module(s).
</P>
<P>
Each module design should include:
<UL>
   <LI> a description of the purpose of this module.</li>
   <LI> complete declarations for all public methods and instance variables.</li>
   <LI> complete declarations for all private methods and instance variables.</li>
   <LI> descriptions of purposes for all methods/routines and meanings of all parameters.</li>
   <LI> descriptions and uses of all non-trivial instance variables and data structures.</li>
   <LI> general descriptions of all non-obvious algorithms.
	<br>
	These need not be pseudo code, but could simply be 
	descriptions of an approach (e.g. "allocate from the heap",
	"bubble-sort" or "compute SHA1 hash").  
	If a perhaps-not-widely-known named
	algorithm is used, provide a reference to a description
	of its implementation (in case the programmer who will 
	reivew or implement it isn't familiar with it).
   	<P>
	A good test for whether or not your design is sufficiently 
	clear and detailed is that if you gave it to one of your
	team mates, they would be able to sit down and write the code.
	</P>
	</li>
   <LI> where the code is to be based on standard libraries,
	call those out (e.g. "synchronization will be with
	boost thread shared locks") and (again, if they are not widely known)
	include a reference to the documentation for the functions to be used.</li>
</UL>
</P>
<P>
If any of these design elements are non-obvious, the rationale 
for those decisions should be described so that the implementer
can better understand what must be done.  People are more likely
to make mistakes when working on things they do not understand.
</P>
<P>
You can prepare your designs in any form you find convenient,
but you may find it easiest to create them as
code modules with compilable declarations, extensive
(e.g. JavaDoc/PyDoc) comments and very little actual code.  
Overview and rationale can be presented as comments in front of the described elements.
Many people choose to start non-trivial implementations by writing the
declarations and comments.
</P>
<P>
If the chosen component is of reasonable size, complexity, and testability
this design be graded on the basis of:
<ul>
	<li> 10% all public methods and instance variables are 
		described well enough to enable a skilled client
		figure out how to use them correctly.</li>
	<li> 20% all implementations and internal methods are 
		described well enough to enable a skilled programmer
		to implement them (with no more research than having
		to read the referenced documentation).</li>
	<li> 10% all variables and data structures are described
		well enough to enable a skilled programmer to use
		and update them correctly.</li>
	<li> 10% the defined methods and instance variables are
		sufficient to fully implement the component interface 
		specification.</li>

	<li> 10% the reasonableness (after having read the rationale)
		of the specified implementations.</li>
	<li> 10% the correctness of the described implementations.</li>
	<li> 10% reasonable exploitation of language and tool capabilities.</li>
	<li>  5% the simplicity (relative to the problem) of the proposed implementations.</li>
	<li>  5% the likely efficiency of the described implementations.</li>

	<li> 10% the readability of the design document, including the
	         clarity and adequacy of the overview and rationale.</li>
</ul>

<P>
Maintain your component design with history (e.g. on Github or in Google Docs).
When you are ready to submit your component design for grading:
<ul>
    <li> create a <a href="#submission">standard submission prologue</a> 
         (followed by your design or a URL to it), and
	 entitle it <tt>design_3c.txt</tt></li>
    <li> up-load it for submission (each person must submit his/her own design)</li>
</ul>
</P>

<a name="desc_test">
<H3>P3B.2 Component Test Plan</h3>
</a>
<P>
Once you have specifications and a design to satisfy them, you have
to figure out how you will test your implementation, to demonstrate
that it actually works.
<UL>
   <LI>
	Review your specifications, and develop a set of automated comprehensive
	(black box) test cases to determine whether or not your component meets 
	its interface requirements and specifications.</li>
	
    <LI>
	Review your design, and consider any possible/plausible component
	misbehavior that would not be detected by the the above-described
	specification-based acceptance tests.
	If you find such, define additional (white box) test cases to exercise
	and validate those behaviors.
	</li>
   <LI>
	Consider your component's interfaces and all of the above test
	cases, and design a means for performing all of those test cases
	in a fully automated fashion ... ideally using an existing
	unit testing framework (to structure your test case implementations,
	invoke them, and collect the results).</li>

   <LI>
	Review the overall list for completeness (is everything covered),
	efficiency (are there redundant tests) and value (are there
	tests for things that don't matter or will never happen), and
	try to optimize out the low-value tests.  If there are no such
	tests, briefly describe the analysis that led you to that conclusion.</li>
</UL>
<P>
It is possible that, in the process of developing your test plan, you will find
assertions that are hard to measure, or functionality that is difficult
to verify (or verify automatically).  If this happens, you may need to
revisit your requirements, specifications and design.
</P>
<P>
The difference between black-box and white-box testing (based on specifications vs implementation)
sounds like a simple binary distinction based on a three-word test ...
but the real world often declines to conform to our simple classifications:
<ul>
   	The notion of black-box is not limited to the specified interfaces for 
	<tt>public</tt> methods.
        If we are testing <tt>private</tt> mutator and accessor functions by storing
	values, retrieving them, and confirming that the expected value was returned,
	the mere fact that we put the word <tt>private</tt> in front of those methods
	does not turn trivial specification-based tests into white-box test cases.</li>
	</p>
	<p>
	But, if a <tt>public</tt> method is required to do complex processing to
	understand the input or create useful output, there may be
	numerous combinations of input parameters that exercise different parts 
	of that code ... and non-trivial analysis may be required to identify
	a set of test cases to exercise all of them.
	The mere fact that the specifications defined the expected input/output
	relationships does not prevent a comprehensive exercise of
	a full range of input combinations from qualifying as white-box testing.
	</p>
	<p>
	Even a method that performs a (nominally) simple function may have to
	do a considerable amount of input validation or handle a wide range
	of possible errors.  These too represent a great many different cases
	to be tested and may require non-trivial analysis to figure out how
	to cause each and confirm correct handling.
	</p>
</ul>
The bottom line is that you have been asked to demonstrate your ability to design
non-trivial software.  I also want you to demonstrate the ability to recognize the
situations that might result in different computations, and to verify the correct
handling of each.
</P>

<P>
Your test plan should include:
<UL>
   <LI>	
	A general overview of the functionality to be verified,
	and the general approach(es) that will be taken to verification.</LI>
   <LI>
	An index of test cases.</LI>
   <LI>
	A description of the framework under which this testing will be performed.</LI>
   <LI>
	A complete list of test cases ... for each:
	<UL>
	    <LI> a brief summary of the assertion to be tested</LI>
	    <LI> the justification (e.g. traceability to specifications or requirements) for including this test case</LI>
	    <LI> any special set-up required to run this test</LI>
	    <LI> how the situation to be tested will be generated</LI>
	    <LI> how correctness of behavior will be ascertained</LI>
	    <LI> any special clean-up required after this test</LI>
	</UL>
    </LI>
</UL>

<P>
Your test plan will be graded on the basis of:
<ul>
	<li> 30% black box tests
	<UL>
	    <li> traceability to specifications</li>
	    <li> reasonableness of test cases</li>
	    <li> completeness of suite</li>
	    <li> form and quality of descriptions</li>
	</UL>
	</LI>
	<li> 30% white box tests
	<UL>
	    <li> traceability to design</li>
	    <li> reasonableness of test cases</li>
	    <li> completeness of suite</li>
	    <li> form and quality of descriptions</li>
	</UL>
	</LI>
	<li> 20% testing framework
	<UL>
	    <li> automated execution and pass/fail determination </li>
	    <li> use of off-the-shelf technology</li>
	    <li> reasonablenss practicality of using this tool for this testing</li>
	</UL>
	</LI>
	<li> 10% overall completeness (how much confidence this suite will give us)</li>
	<li> 10% overall efficiency (low value or redundant tests, overly expensive executions)</li>
</ul>

<P>
Maintain your tes plan with history (e.g. on Github or in Google Docs).
When you are ready to submit your test plan for grading:
<ul>
    <li> create a <a href="#submission">standard submission prologue</a> 
         (followed by your test plan or a URL to it), and
	 entitle it <tt>test_3c.txt</tt></li>
    <li> up-load it for submission (each person must submit his/her own test plan)</li>
</ul>
</P>

<a name="desc_review3">
<H2>Design and Test Plan Reviews</H2>
</a>
<P>
Because your component has some algorithmic complexity and requires a non-trivial number
of test cases your design and test plans should be submitted for review.
In Project 2 we required you to follow a fairly formal process (with another team
supplying the facilitator and scribe).  This is a simpler design, entirely appropriate
to be reviewed by other members of your team (all of whom should already be well-familiar
with what your component does).
You are welcome, for this less formal process, to act as the facilitator and scribe 
for your the review of your own component.
But the other basic rules (e.g. about content, scope and behavior) still apply:
<ul>
   <li> the committee should still vote on the resolution (must fix, should fix, comment)
        of each issue/observation.</li>
   <li> a report, covering all non-trivial issues and their resolutions must be
        produced <u>and approved by the reviewing team</u>.
</ul>
</P>
<P>
Each member of the team will submit his/her preliminary specifications,
design and test plans for review by the one or more other team members.  Each
team member will participate in the reviews of the designs and plans
submitted by other members of his/her team.
</P>
<a name="desc_notes3">
<H3>P3C.1 Review Notes</h3>
</a>
<P>
Prior to each review meeting, each of you (individually) will read
the submitted specifications, designs, and test plan and prepare detailed notes
on all questions and concerns.  These notes must be submitted at
least 24 hours prior to the actual review session.  They should be
neat notes, describing legitimate issues clear enough to be sent
as email, and organized for discussion (e.g. in a reasonable 
review order).
</P>

<P>
Each set of review notes will be graded on the basis of:
<ul>
	<li> 40% submitted 24 hours before scheduled review</li>
	<li> 30% the thoroughness of study to which they attest</li>
	<li> 10% how well articulated the issues are</li>
	<li> 10% all comments appropriate and within scope</li>
	<li> 10% issues reasonably organized for discussion</li>
</ul>

<P>
When your notes are eady for submission and grading:
<ul>
    <li> prepare your notes (ideally ASCII text in a file named <tt>notes_3d.txt</tt>).
	 If you are reviewing multiple components, it would be best if you combined
	 all of those notes into a single submission.</li>
    <li> put a <a href="#submission">standard submission prologue</a> on the front of the file</li>
    <li> up-load it for submission (each person must submit his/her own notes)</li>
</ul>
</P>
<a name="desc_report3">
<H3>P3C.2 Review and Report</h3>
</a>
<P>
You will conduct design reviews for each submitted
Specification/Design/Test Plan package.  The process will similar
to the architectural review ... but because this is 
simpler and you have already followed this process 
(for your architectural reviews) these
reviews will not be observed and graded.  
</P>
<P>
You have much more latitude in these reviews:
<ul>
   <li> you can have the entire team review each person's proposal,
        or you can break it up into smaller groups for each review.</li>
   <li> you can have a scribe (who is not the owner of the module
        being reviewed) who writes and submits the report, or the
	module owner can act as the scribe and write up the report
	for his/her own review.</li>
</ul>
But each person must create and submit notes for (at least) one review, 
and must write up a report for one review.
</P>
<P>
As with the architectural review,
the report is not "meeting minutes".  Rather it is a distillation of key issues and
decisions.  It must contain:
<UL>
   <LI> the time, date, and attendees</li>
   <LI> the component to be reviewed</li>
   <LI> a clear summary of each important issue rased</li>
   <LI> a characterization as a defect, question, or issue</li>
   <LI> a characterization as must-fix, should-fix, or comment</li>
   <LI> a disposition for the entire proposal of
   <UL>
	<li> approved</li>
	<li> approved with required changes</li>
	<li> requires another meeting</li>
	<li> rejected (this cannot be made to work)</li>
   </UL>
   </LI>
</UL>
</P>
<P>
Each review report will be graded on the basis of:
<ul>
	<li> 10% form: time, place, project, attendees</li>
	<li> 20% scope of the report (just issues, no opinions)</li>

	<li> 50% clarity with which issues are presented</li>
	<li> 10% clear disposition for each issue</li>
	<li> 10% disposition of the entire project</li>
</ul>

<P>
When your review report is eady for submission and grading:
<ul>
    <li> create a <a href="#submission">standard submission prologue</a> 
         (followed by your review report or a URL to it), and
	 entitle it <tt>review_3d.txt</tt></li>
    <li> up-load it for submission (each person must submit his/her own review report)</li>
</ul>
</P>

<a name="desc_final3">
<H2>P3D Final Component Specifications (P3D.1), Design (P3D.2) and Test Plan (P3D.3)</h2>
<P>
Note: this is likely to be a relatively light week for project work.
      You would be well advised to use this opportunity to get an
      early start on the project 4 implementation, which probably 
      comes due a the same time as the final projects in your other classes.
</P>
</a>
<P>
It is likely that your design and test case development, and their
reviews will turn up issues that require changes to your specifications, 
design and test plan.  
Address those issues (by revising your specifications, design and test plan), 
document the changes that were made to address each, and get agreement from the 
reviewers that the issues have been satisfactorally addressed.
</P>
<P>
The primary parts of the final design submission are:
<UL>
   <LI> an updated component specification.</LI>
   <LI> an updated updated component design.</LI>
   <LI> an updated test plan.</LI>
</UL>
Each of thes should include a discussion of the issues that were 
discovered (since the preliminary submissions), and a summary of
the changes that have been made, and the reasons for each.
</P>
<P>
This submission be graded on the basis of:
<ul>
	<li> 15% quality and completness of the final specifications,
	         including responses to issues identified in the
		 reviews and grading of the earlier versions.</li>
	<li> 30% quality and completeness of the final design,
	         including responses to issues identified in the
		 reviews and grading of the earlier versions.</li>
	<li> 15% extent to which the final design meets the
	 	 <em>algorithmically interesting</em> requirement
		 (you will be implementing a significant amount of
		 non-trivial code).</li>
	<li> 25% quality and completeness of the final test plan,
	         including responses to issues identified in the
		 reviews and grading of the earlier versions.</li>
	<li> 15% extent to which the final test plan meets the
		 requirements for fully automated testability, and 
		 significant tests based on design/implementation 
		 rather than interface specifications.</li>
</ul>
</P>
<P>
When you have addressed all of the issues raised in your review and are
ready to submit your final component specifications, design and plan,
<ul>
    <li> prepare your spec (ideally ASCII text in a file named <tt>spec_3e.txt</tt>)</li>
    <li> prepare your design (ideally ASCII text in a file named <tt>design_3e.txt</tt>,
         or perhaps with some other language-specifc suffix)</li>
    <li> prepare your test plan (ideally ASCII text in a file named <tt>test_3e.txt</tt>)</li>
    <li> put a <a href="#submission">standard submission prologue</a> on the front of each file</li>
    <li> up-load them for submission (each person must submit his/her own files)</li>
</ul>
</P>

<a name="desc_3pm">
<H3>P3D.4 Post-Mortem Report</H3>
</a>
<P>
This project is a learning exercise, and one of the major ways
we learn is by analyzing past mistakes.  You will, as a team,
review all aspects of this project.  One of you will then 
summarize that process into a post-mortem analysis report.
</P>
<P>
A report, summarizing the key issues raised in your post-mortem,
and the conclusions you came to.  Your post-mortem discussion 
should include:
<ul>
	<li> architectural refinement and specification development.</li>
	<li> development of the component designs.</li>
	<li> development of the test plans.</li>
	<li> the review process and the resulting design/plan changes.</li>
	<li> the planning and ongoing management of these activities.</li>
	<li> the overall project as an educational exercise.</li>
</ul>
</P>
<P>
The submission and grading of Post Mortem reports is described
in the <a href="#postmortem">General Grading</a> information.
<P>
Make sure that you have kept your meeting minutes and management plan up-to-date on Github.
When you are ready to submit the Post-Mortem report (and management notes) for grading:
<ul>
    <li> prepare your report (ideally ASCII text in a file named <tt>postmortem_3.txt</tt>)</li>
    <li> put a <a href="#submission">standard submission prologue</a> on the front of it</li>
    <li> make sure that it also contains Github URLs for your status updates and management plans</li>
    <li> up-load it for submission (only one person on the team needs to do this)</li>
</ul>
</P>
<P>
This report be graded on the basis of:
<ul>
	<li> 50% whether or not you meaningfully discuss each of the required activities.</li>
	<li> 20% whether or not you identify all of the important incidents.</li>
	<li> 30% the extent to which you are able to derive and articulate useful lessons
	     (and good future advice) from those experiences.</li>
</ul>
</body></html>
