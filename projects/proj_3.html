<html><head>
<title>Specs & Design</title>
</head><body>
<center>
<a name="desc_3">
<h1>Project Phase 3<br>
</a>
Specifications, Design and Review
</h1>
</center>

<H2>Introduction</h2>
<P>
Thus far, most of your work has been team activities.  Moving forward,
you will do much of your work as individuals, tho still with considerable 
coordination, collaboration, and assistance from other team members.
</P>
<P>
The architecture has described the roles, functionality, and interfaces of each of
the key components in our system.  A few components may be simple enough
that we can simply sit down, code them up, and watch them work ... but most
components are more complex than that.  Before we start coding a non-trivial
component we need to make sure that:
<ul>
   <li> we understand (completely and in detail) what our component will do.</li>
   <li> we understand how we will implement those things.</li>
   <li> we understand how we will test the correctness of that implementation.</li>
   <li> we have not made any obvious mistakes in our plans for the above.</li>
</ul>
In this project, we will complete our implementation prerequisites.
</P>
<P>
In this phase you will create specifications, designs and testing plans
for chosen components.
In the next (and final) phase you will execute these plans, building, 
testing, integrating, and demonstrating working software.
I suggest that you review the work you will have to do in projects 3 and 4,
and then give considerable thought to which components (or parts of which
components) you want to choose.
Depending on your architecture, these could each be a complete architectural component,
or small pieces (e.g. an applet or a few classes) from a single architectural component,
or even from multiple components:
</P>
<UL>
   <li> each of the chosen pieces should be somewhere in the range of 
        100-400 lines of code.</li>
   <li> each chosen piece <u>must</u> implement an <em>algorithmically interesting</em>
	(much more than initializers, mutators, accessors, and routines that
	pass a request on to another service) program, module, or class.
	</li>
   <li> each chosen piece <u>must</u> have sufficiently complex behavior to require
   	a significant number of test cases ... including several
	white-box test cases (that go beyond simple verification vs interface
	specifications).</li>
   <li> each person should be capable of <u>independently</u> building and 
        testing his/her chosen piece.</li>
   <li> each of these pieces <u>must be automatically</u> unit-testable.</li>
   <li> when you are done, it <u>should</u> be possible to combine all
	of these individual pieces together to create a working 
	(all the pieces working together) and demo-able component.</li>
</UL>
Note, however, that you are each implementing only one (or a few) module(s) ...
just enough to require a reasonable amount of design and coding work, and 
a modest number of test cases.  It is not required that the sum of these 
modules must add up to the whole of your proposed product.
Neither must each of you implement a <u>single complete</u> architectural component:
<ul>
    <li> if a single component in your architecture is likely to be
    	 on the order of 400-800 lines of code, you can break it
	 up into multiple classes, and implement only a few of them.</li>
    <li> if you very much want to implement a component that is too simple
         (because of the value it adds to your program), you
    	 may need to choose multiple components to implement.  But if
	 none of those components meet the complexity and test case
	 requirements, you may have trouble earning full credit
	 in projects 3 and 4.
	 </li>
</ul>
It is very important that you consider these requirements when choosing
the components to be implemented.  
It would be unfortunate of you chose components A, B, and C, and only later
realized:
<UL>
   <li> component A was too large or required the use of new tools
   	that would take weeks to master.</li>
   <li> component B was too simple to earn full points for the
   	design and/or test plan.</li>
   <li> component C cannot be tested (or even built) until after
        components A and B are complete and working.</li>
   <li> it would be very difficult to show, at the end of the final
   	integration, these three components working together.</li>
</UL>
Give some thought to how each component could be (independently) built
and tested, and how you might demonstrate their successful integration.
If you have any questions or doubts, please talk to me or your grutor before
finalizing your decisions.
</P>
<P>
Your pieces can be implemented in any appropriate language or combination of languages, 
and use any tool-kits or middle-ware you find convenient ... but it must be 
compilable/executable code with some algorithmic complexity
(much more than U/I widgets, data, HTML or images),
and must be accompanied by a fully automated unit test suite.
</P>
<P>
The warning about choosing U/I components is based on two concerns:
<ol type="1">
   <li> Putting up U/I widgets and responding to their call-backs is
	so simple that it is commonly assigned as a project in introductory
	programming classes.</li>
   <li> If the primary inputs to your component are touches/clicks,
        and your primary outputs are pixels on the screen
	<ol type="a">
	    <li>it may be difficult to create a comprehensive automated unit-test suite</li>
	    <li> most of the code being tested is not yours, but the GUI tool kit.</li>
	</ol>
</ol>
But this does not mean that it is impossible for U/I components to satisfy the
design and implementation requirements:
<ul>
   <li> If your U/I component does significant analysis of the input
        (e.g. syntax parsing, keyword recognition, validity checking,
	carrying out complex request protocols)
	such processing might qualify as both non-trivial and auto-testable.</li>
   <li> The same might be the case if you have to do signficant processing
        (e.g. turn partially-ordered pairs into a visual tree) to filter and transform
	complex data into a form that is usefully presentable to the intended user.</li>
   <li> Some U/I toolkits incorporate unit-testing frameworks, that
        permit user selections to be simulatable (e.g. <tt>click_button("Select File")</tt>)
	and results (to be displayed) tested by examining object state
	(e.g. <tt>chosen_file.getText()</tt>).</li>
</ul>
If your U/I is rich
enough in functionality (e.g. several hundered lines of code
involving complex widgets and event processing), does much
error checking, and and is thoroughly exercisable with such tools, 
it could easily qualify as a component for design and implementation
in the next phases of the project.
If you believe you have a U/I component that would qualify, review it with
me before completing your selections and plans.
</P>
</P>
<P>
The first draft designs and test plans for the chosen components are due in the second week
of this project ... but you will be defining the interfaces
you will be exporting for use by other team members this week.
These interfaces represent critical inter-dependencies between otherwise individual
development efforts, and must be negotiated between the producers and consumers.
<ol type="a">
   <li>	Sketching out and discussing those interfaces up-front will make it much easier
   	to pursue your (individual component) design activities next week.</li>
   <li> As you pursue your own designs, you will likely come to a better understanding
        of what functionality (and interfaces) you need from the other components with
	which you interact.</li>
</ol>
Thus, component interfaces must be well specified before you begin your detailed designs,
but it is quite likely that those interfaces will change as a result of understandings
gained in the process of doing those detailed designs.
Even though detailed designs are not due until the next phase, you should already have
some pretty good ideas about how each component will be implemented.  If not, you may
find that you have specified something that cannot be built ... 
and all of the work done based on that specification may wind up being thrown away.
</P>
<P>
There are multiple phases to this project, each of which has its
own goals, processes, and deliverables (most of which are individual rather than team):

<IMG src="proj_3.jpg">

<table align="center" border="1" cellpadding="5" cellspacing="0">

<tbody>
  <tr>
	<th>Phase</th>
	<th>Assignment</th>
	<th>Value<sup>3</sup></th>
  </tr>

  <tr>
	<td rowspan="2"> 3A </td>
	<td> <A href=#desc_plan3>Plan</a> (for your component)</td>
	<td> 5 </td>
  </tr>

  <tr>
	<td> <A href=#desc_spec>Component Specifications</a> (for your component)</td>
	<td> 10 </td>
  </tr>
  <tr>
	<td rowspan="2"> 3B </td>
	<td> <A href=#desc_dsgn>Component Design</a> (for your component)</td>
	<td> 20 </td>
  </tr>
  <tr>
	<td> <A href=#desc_test>Component Test Plan</a> (for your component)</td>
	<td> 20 </td>
  </tr>

  <tr>
	<td rowspan="2"> 3C </td> 
	<td> <A href=#desc_notes3>Review Notes</a> (you prepared for other reviews) </td>
	<td> 10 </td>
  </tr>

  <tr>
	<td> <A href=#desc_report3>Review Report</a> (from review of your component)</td>
	<td> 5 </td>
  </tr>

  <tr>
	<td rowspan="2"> 3D </td>
	<td> <A href=#desc_final3>Final Specifications, Design and Test Plan</a> (for your component)</td>
	<td> 20 </td>
  </tr>

  <tr>
	<td> <A href=#desc_3pm>Post-Mortem Report</a> (team)</td>
	<td> 10 </td>
  </tr>
</tbody>
</table>
<UL>
<sup>3</sup> less 10% for each <a href=#latepolicy>unexcused</a> late day.<br>
</UL>
<a name="desc_plan3">
<H2>P3A.1 Plan</h2>
</a>
<P>
Perhaps the most important part of your plan is which components or classes
each of you will implement.
But each team will prepare a task-breakdown, identify the dependency relationships between
tasks (and components), and owners for each sub-task, assign due-dates, and schedule regular reviews
of both work-products and progress (to enable adequate time to deal with the
<u>problems that will arise</u>).  
</P>
<P>
Most of the work involved in creating your specifications, designs, and test
plans will be individual.  But team-mates will still have significant
dependencies on one another:
<ul>
   <li> the specifications (and changes to them) must
	be agreed upon by the producers and consumers.</li>
   <li> the design reviews must be accomplished within
   	a short period of time, which means that reviewers
	must be lined up in advance and the documents to
	be reviewed must be in good form prior to the 
	scheduled review dates.</li>
</ul>
Even though much of this work will be done individually, 
the team must agree on (and meet) a schedule for when
materials will be ready and reviews will happen.
A slip in one person's schedule may cause delays for
team-mates who are depending on those results.
One of the advantages of team activities is that
regular meetings (e.g. <em>daily stand-ups</em>) keep
us on schedule.
Thus, you are encouraged to continue having regular status
updates and to maintain a <tt>minutes.txt</tt> file (in your repo or GoogleDocs).
</P>
<P>
The amount of work required to refine your architecture to the point that
it is possible to identify and specify your chosen components will vary
greatly from one team/product to the next, and I would encourage you to get
this behind you as quickly as possible.  Once you have a sense of what
the chosen components are, you should have a pretty good idea of how much
work it will be to do the designs and test plans.  You should, however,
leave yourself ample time for discovering issues with the chosen components,
the initial designs, and making the required changes.
</P>
<P>
As your understanding of the problem
evolves and you respond to unanticipated events, 
you will have to revise your plan (not merely estimates, but
the work to be done).  Make sure that you document each of these
problems and the manner in which you decide to respond to it.
If deadlines are missed, or deliverables fail to pass review, the fact, as well
as the causes and the plan to remedy them must be documented.
</P>
<P>
Your initial Management Plan will be graded on the basis of:
<ul>
	<li> 20% well chosen components (in terms of size, complexity, testability)</li>
	<li> 20% good use of time and resources (work spread reasonably over the available time)</li>
	<li> 20% specificity of plan (clear responsibilities: what, when)</li>
	<li> 20% provisions for early detection of problems, and time to deal with them</li>
	<li> 20% completeness</li>
</ul>
<P>
Maintain your plan (and status update minutes) with history (e.g. on Github or in Google Docs).
You will probably be updating them daily, and we will be reviewing this history.
</P>
<P>
When you are ready to submit your plan for grading:
<ul>
    <li> prepare your management plan (<tt>management_3a</tt> with an appropriate suffix)</li>
    <li> put a <a href="#submission">standard submission prologue</a> on the front of it</li>
    <li> up-load it for submission (each person must submit his/her own plan)</li>
</ul>
</P>

<a name="desc_spec">
<H2>P3A.2 Specifications</h2>
</a>
<P>
Each team member will take ownership of one or more modules (or components).
To prepare a specifications, you will:
<OL type="1">
  <li> (perhaps) expand the architecture (above the chosen components)
        to describe them and the components with which they interact.
	If you have decided to implement pieces of your system that
	are smaller than complete architectural components (from project 2), 
	you will have to expand and refine that architecture down to the 
	level of the pieces you want to use for projects 3 and 4.  Note that
	you do not have to expand everything in your architecture to this
	level of detail.  You only have to do a top-down refinement along
	the path to the components you will be using for this project.
	<p>
	If a component to be implemented was already <u>fully described</u>
	(detailed specifications for all external interfaces/public methods)
	in the (Project 2) architecture, no further expansion is required.
	If further top-down refinement is required, your component specifications
	should be accompanied by addenda to the (submitted for project 2) architecture.
	It may be possible that a single addendum (created by the entire team)
	could be used for all of the component specifications.
    	</p>
	</li>
  <li> generally describe the functionality of each of the component(s)
	to be designed, and their role(s) in the overall architecture.</li>
  <li> determine the requirements to be imposed on each of the component(s) to
      	be designed, based on the product requirements 
	and the components' roles in the overall architecture.</li>
  <li> enumerate all of the external interfaces/public methods (in both directions)
   	between the component(s) to be designed and the rest of the system.</li>
  <li> write a <u>complete specification</u> (both form and function,
       detailed enough to define acceptance criteria)
       for all of the external interfaces/public methods to the chosen components.
       </li>
</OL>
<P>
There is no universal definition for what "specifications" should describe,
in how much detail.  The answers to those questions depend on:
<ul>
   <li>what is being described</li>
   <li>to whom</li>
   <li>for what purpose</li>
</ul>
</P>
In this case, we are asking you to describe
<ul>
   <li> the external entrypoints (public methods) to be exposed by your component
   	<ul>
	    <li>the (exact) name of the method</li>
	    <li>the name, type, and meaning of each parameter,
	        and any validity assertions that might apply to it</li>
	    <li>a functional description of everything that is expected to
	        happen as a result of having called this method</li>
	    <li>the type(s) and meaning(s) of the return value(s)</li>
	</ul>
   </li>
   <li> to your team members and your grader (who have a reasonable familiarity with your project
        and the toolkit(s) to be used in its construction) </li>
   <li> it is intended that these specifications be sufficiently detailed:
        <ul>
   	    <li>to enable other people to write correct code to call those entrypoints 
	         and use the services this component provides</li>
	    <li>to provide an adequate specification for developing a complete set
	        of black-box test cases</li>
	    <li>to completely specify all of the requirements that must be met by
	        the (coming soon) design and implementation.</li>
        </ul>
    </li>
</ul>
<P>
It is possible that your component will have no requirements beyond delivering above-described
functionality.  But it is likely that, as a result of its role in the overall architecture,
and the things that other components need it to do, there will be additional requirements
imposed on the manner in which your component provides its functionality or cases that must
be handled.
</P>
<P>
How can you know if your specifications are adequate?
<UL>
    Give them to your team-mates and ask your team-mates if they 
    now know <u>everything</u> they need to know in order to correctly
    write code that uses the services of your component.
    If anyone still has any questions, the specifications should be updated
    to address them.
</UL>
</P>
<P>
Specifications can be written as prose, but we are probably describing class 
and method APIs.  The most obvious form for representing APIs is as 
(class, method and field) declarations (in the language in which they will be
implemented) with <em>Docstring</em> comments to describe them:
<UL>
    <li>this will be understandable by the intended audience (other programmers)</li>
    <li>such declarations are unambiguous, and can be checked by the compiler</li>
    <li>this creates a template for the more detailed design and code that will follow</li>
    <li>readily available tools can process this code and automatically generate
        both API documentation and class diagrams.</li>
</UL>
You are free to choose the format that best tells your story, but you can find a
<a href="examples/overview.html">sample</a>
of such specifications and designs on the course web site.
This URL points to an architectural overview for the described component,
and that contains links to sample specifications, designs, and test plans.
</P>
<P>
This submission be graded on the basis of:
<ul>
	<li> 30% clarity and reasonableness of component's functionality and role in the overall system,
	         and its suitability (in terms of size, complexity, testability) for the assignment.</li>
	<li> 20% completeness, clarity and measurability of requirements: 
	     <ul>
	         <li>derived from the architecture and overall product requirements</li>
		 <li>new requirements arising from the component definition</li>
	     </ul>
	     </li>
	<li> 10% spec covers all required functionality in this module</li>
	<li> 10% clear and complete (ready to compile) declarations for all public methods 
	     and instance variables</li>
	<li> 10% clear and complete (ready to code) functionality descriptions for all public methods</li>
	<li> 10% well abstracted module functionality, and reasonably chosen methods and parameters</li>
	<li> 10% interface definitions lend themselves to automated compliance testing</li>
</ul>
<P>
Maintain your specifications with history (e.g. on Github or in Google Docs).
When you are ready to submit your component specifications for grading:
<ul>
    <li> create a <a href="#submission">standard submission prologue</a> 
         (that includes your specifications or a URL to them), and
	 entitle it <tt>spec_3b.txt</tt></li>
    <li> if you need to expand your (project 2) architectural description, create an
         addendum (only the changes, not a whole new copy) and either include those
	 changes (or a URL to them) in front of your specifications.</li>
    <li> up-load it for submission (each person must submit his/her own specifications)</li>
</ul>
</P>
<a name="desc_dsgn">
<H2>Design and Test Plan</H2>
<H3>P3B.1 Component Design</h3>
</a>
<P>
Each team member will prepare a detailed design for one or 
more modules, ideally comprising 100-400 lines of code when complete.  
This design need not be at the level of complete pseudo-code, but 
(in combination with the specifications) should be sufficient to 
enable a skilled programmer to easily and correctly implement the specified module(s).
</P>
<P>
Each module design should include:
<UL>
   <LI> a prose description of the purpose of this module.</li>
   <LI> complete descriptions for all public methods
   	(name, purpose, parameters, return values, functionality) 
	and instance variables (name, meaning, type, notes on initialization and usage).
   <LI> If, in the process of doing the detailed design, you recognize that you will
	want to create private methods (e.g. to perform commonly used functions
	or encapsulate complex operations) and/or instance data, those too 
	should be described.
   </li>
   <LI> general descriptions of all non-obvious algorithms.
	<br>
	These need not be pseudo code, but could simply be 
	descriptions of an approach (e.g. "allocate from the heap",
	"bubble-sort" or "compute SHA1 hash").  
	If a perhaps-not-widely-known named
	algorithm is used, provide a reference to a description
	of its implementation (in case the programmer who will 
	reivew or implement it isn't familiar with it).
   	<P>
	A good test for whether or not your design is sufficiently 
	clear and detailed is that if you gave it to one of your
	team mates, they would be able to sit down and write the code.
	</P>
	</li>
   <LI> where the code is to be based on standard libraries,
	call those out (e.g. "synchronization will be with
	Boost Thread shared locks") and (again, if they are not widely known)
	include a reference to the documentation for the functions to be used.</li>
</UL>
</P>
<P>
If any of these design elements are non-obvious, the rationale 
for those decisions should be described so that the implementer
can better understand what must be done.  People are more likely
to make mistakes when working on things they do not understand.
</P>
<P>
You can prepare your designs in any form you find convenient,
but you may find it easiest to create them as
code modules with compilable declarations (complemented by full JavaDoc/PyDoc tags),
and algorithmic comments (rather than code).
Overview and rationale can be presented as comments in front of the described elements.
Many people regularly do most of their designs in this way:
<ul>
   <li> the design documentation becomes a template for the code that
        will be added later.</li>
   <li> even before the code bodies are filled in, the compiler can
   	check for the correctness of the declarations, and 
	inter-module consistency.</li>
   <li> when the code is added, it will be inseparable from the design
        documentation ... which will help future developers understand
	the code, and make it much easier to keep the design documentation
	up-to-date as the code evolves.</li>
   <li> automated tools (e.g. Doxygen/PyDoc/umlgraph) can be used to automatically
   	extract this information from the code and automatically generate
	<em>class diagrams</em> and standard format documentation
	(and thus eliminate the need for you to manually create much of the prose documentation).
	</li>
</ul>
</P>
<P>
<u>If the chosen component is of reasonable size, complexity, and testability</u>
this design be graded on the basis of:
<ul>
	<li> 10% the defined methods and instance variables are
		sufficient to fully implement the component interface 
		specification.</li>

	<li> 10% all methods (including internal) well enough specified
		to enable a skilled programmer (other than the designer)
		to write code to correctly use them.</li>
	<li> 20% all methods (including internal) are well enough described
		to enable a skilled programmer (other than the designer)
		to implement them (with no more research than having
		to read the referenced documentation).</li>
	<li> 10% all variables and data structures (including internal) 
		are described well enough to enable a skilled programmer to use
		and update them correctly.</li>

	<li> 10% the correctness (likelihood of working) of the described implementations.</li>
	<li> 10% the reasonableness (after having read the rationale)
		of the describged implementations.</li>
	<li> 10% reasonable exploitation of language and tool capabilities.</li>
	<li>  5% the simplicity (relative to the problem) of the proposed implementations.</li>
	<li>  5% the likely efficiency of the described implementations.</li>


	<li> 10% the readability of the design document, including the
	         clarity and adequacy of the overview and rationale
		 to provide the required background.</li>
</ul>

<P>
Maintain your component design with history (e.g. on Github or in Google Docs).
When you are ready to submit your component design for grading:
<ul>
    <li> create a <a href="#submission">standard submission prologue</a> 
         (followed by your design or a URL to it), and
	 entitle it <tt>design_3c</tt> (with a type-appropriate suffix)</li>
    <li> up-load it for submission (each person must submit his/her own design)</li>
</ul>
</P>

<a name="desc_test">
<H3>P3B.2 Component Test Plan</h3>
</a>
<P>
Once you have specifications and a design to satisfy them, you have
to figure out how you will test your implementation, to demonstrate
that it actually works.
<UL>
   <LI>
	Review your specifications, and develop a set of automated comprehensive
	(black box) test cases to determine whether or not your component meets 
	its interface requirements and specifications.</li>
    <LI>
	Review your design, consider special cases (or combinations
	of cases) that would exercise different code branches, and develop
	a set of (white box) test cases to demonstrate that each interesting
	situation is handled correctly.
	</li>
    <LI>
	Review your design, and consider any possible/plausible component
	misbehavior that would not be detected by the the above-described
	specification-based acceptance tests.
	If you find such, define additional (white box) test cases to exercise
	and validate those behaviors.
	</li>
   <LI>
	Consider your component's interfaces and all of the above test
	cases, and design a means for performing all of those test cases
	in a fully automated fashion ... ideally using an existing
	unit testing framework (to structure your test case implementations,
	invoke them, and collect the results).</li>

   <LI>
	Review the overall list for completeness (is everything covered),
	efficiency (are there redundant tests) and value (are there
	tests for things that don't matter or will never happen), and
	try to optimize out the low-value tests.  If there are no such
	tests, briefly describe the analysis that led you to that conclusion.</li>
</UL>
<P>
It is possible that, in the process of developing your test plan, you will find
assertions that are hard to measure, or functionality that is difficult
to verify (or verify automatically).  If this happens, you may need to
revisit your requirements, specifications and design.
Please feel free to ask for help if you find yourself in such a situation.
</P>
<P>
The difference between black-box and white-box testing (based on specifications vs implementation)
sounds like a simple binary distinction based on a three-word test ...
but the real world often declines to conform to our simple classifications:
<ul>
   	The notion of black-box is not limited to the specified interfaces for 
	<tt>public</tt> methods.
        If we are testing <tt>private</tt> mutator and accessor functions by storing
	values, retrieving them, and confirming that the expected value was returned,
	the mere fact that we put the word <tt>private</tt> in front of those methods
	does not turn trivial specification-based tests into white-box test cases.</li>
	</p>
	<p>
	But, if a <tt>public</tt> method is required to do complex processing to
	understand the input or create useful output, there may be
	numerous combinations of input parameters that exercise different parts 
	of that code ... and non-trivial analysis may be required to identify
	a set of test cases to exercise all of them.
	The mere fact that the specifications defined the expected input/output
	relationships does not prevent a comprehensive exercise of
	a full range of input combinations from qualifying as white-box testing.
	</p>
	<p>
	Even a method that performs a (nominally) simple function may have to
	do a considerable amount of input validation or handle a wide range
	of possible errors.  These too represent a great many different cases
	to be tested and may require non-trivial analysis to figure out how
	to cause each, and confirm correct handling.
	</p>
</ul>
The bottom line is that you have been asked to demonstrate your ability to design
non-trivial software.  I also want you to demonstrate the ability to 
<u>recognize situations that might result in different computations</u>,
and to <u>devise test cases to verify the correct handling of each</u>.
</P>

<P>
In general, your test plan should include:
<UL>
   <LI>	
	A general overview of the functionality to be verified,
	and the general approach(es) that will be taken to verification.</LI>
   <LI>
	An index of test cases.</LI>
   <LI>
	A description of the framework under which this testing will be performed.</LI>
   <LI>
	A complete list of test cases ... for each:
	<UL>
	    <LI> a brief summary of the assertion to be tested</LI>
	    <LI> the justification (e.g. traceability to specifications or requirements) for including this test case</LI>
	    <LI> any special set-up required to run this test</LI>
	    <LI> how the situation to be tested will be generated</LI>
	    <LI> how correctness of behavior will be ascertained</LI>
	    <LI> any special clean-up required after this test</LI>
	</UL>
    </LI>
</UL>
<P>
This could be a fair amount of information. 
Please do not feel you have to give me a half-page of 
prose for each of a two-digit-number of test cases.
<ul>
   <li>to the extent that different test cases are merely different parameter values
       (or entries in a table), the set-ups do not have to be written out in English prose;
       The different test cases can be enumerated by entries in a table.</li>
   <li>the assertions to be tested need not be written out in English prose,
       but need only be understandable by a programmer.
       </li>
</ul>
The following is an example of a concise representation of test cases, where
the assertions being tested are obvious from the names, set-ups and
expected result.
</P>
<center>
<TABLE frame="box" rules="all">
<TBODY>
	<TR>
	    <TH>Name</TH> <TH>Set-up</TH> <TH>Test</TH> <TH>Expected Result</TH>
	</TR>
	<TR>
	    <TD>passwd-correct</td>
	    <TD>dbase: USER1 w/password PASSWD1</td>
	    <TD>command: LOGIN USER1 PW=PASSWD1</td>
	    <TD>response = 200 (success)</td>
	</TR>
	<TR>
	    <TD>passwd-incorrect</td>
	    <TD>dbase: USER1 w/password PASSWD1</td>
	    <TD>command: LOGIN USER1 PW=PASSWD2</td>
	    <TD>response = 510 (failure)</td>
	</TR>
	<TR>
	    <TD>passwd-badname</td>
	    <TD>dbase: no such user as XXX</TD>
	    <TD>command: LOGIN XXX PW=XXX</td>
	    <TD>response = 511 (failure)</td>
	</TR>
	<TR>
	    <TD>auth-user-OK</td>
	    <TD>dbase: USER1 as a normal user</TD>
	    <TD>logged in as USER1<br>command: STATUS USER1</td>
	    <TD>response = 200 (success)</td>
	</TR>
	<TR>
	    <TD>auth-user-priv</td>
	    <TD>dbase: USER1 as normal user<br>
	        dbase: USER2 as normal user</TD>
	    <TD>logged in as USER1<br>command: STATUS USER2</td>
	    <TD>response = 523 (not allowed)</td>
	</TR>
	<TR>
	    <TD>auth-manager-priv</td>
	    <TD>dbase: MGR1 as manager<br>
	        dbase: USER2 as normal user</TD>
	    <TD>logged in as MGR1<br>command: STATUS USER2</td>
	    <TD>response = 200 (success)</td>
	</TR>
</TBODY>
</TABLE>
</center>

<P>
Your test plan will be graded on the basis of:
<ul>
	<li> 30% black box tests
	<UL>
	    <li> traceability to specifications</li>
	    <li> reasonableness of test cases</li>
	    <li> completeness of suite</li>
	    <li> form and quality of descriptions</li>
	</UL>
	</LI>
	<li> 30% white box tests
	<UL>
	    <li> traceability to design</li>
	    <li> reasonableness of test cases</li>
	    <li> completeness of suite</li>
	    <li> form and quality of descriptions</li>
	</UL>
	</LI>
	<li> 20% testing framework
	<UL>
	    <li> automated execution and pass/fail determination </li>
	    <li> use of off-the-shelf technology</li>
	    <li> reasonablenss practicality of using this tool for this testing</li>
	</UL>
	</LI>
	<li> 10% overall completeness (how much confidence this suite will give us)</li>
	<li> 10% overall efficiency (low value or redundant tests, overly expensive executions)</li>
</ul>

<P>
Maintain your test plan with history (e.g. on Github or in Google Docs).
When you are ready to submit your test plan for grading:
<ul>
    <li> create a <a href="#submission">standard submission prologue</a> 
         (followed by your test plan or a URL to it), and
	 entitle it <tt>test_3c</tt> (with an appropriate suffix)</li>
    <li> up-load it for submission (each person must submit his/her own test plan)</li>
</ul>
</P>

<a name="desc_review3">
<H2>Design and Test Plan Reviews</H2>
</a>
<P>
Because your component has some algorithmic complexity and requires a non-trivial number
of test cases your design and test plans should be submitted for review.
In Project 2 we required you to follow a fairly formal process (with another team
supplying the facilitator and scribe).  This is a simpler design, entirely appropriate
to be reviewed by other members of your team (all of whom should already be well-familiar
with what your component does).
You are welcome, for this less formal process, to act as the facilitator and scribe 
for your the review of your own component.
But the other basic rules (e.g. about content, scope and behavior) still apply:
<ul>
   <li> the committee should still vote on the resolution (must fix, should fix, comment)
        of each issue/observation.</li>
   <li> a report, covering all non-trivial issues and their resolutions must be
        produced <u>and approved by the reviewing team</u>.
</ul>
</P>
<P>
Each member of the team will submit his/her preliminary specifications,
design and test plans for review by the one or more other team members.  Each
team member will participate in the reviews of the designs and plans
submitted by other members of his/her team.
</P>
<a name="desc_notes3">
<H3>P3C.1 Review Notes</h3>
</a>
<P>
Prior to each review meeting, each of you (individually) will read
the submitted specifications, designs, and test plan and prepare detailed notes
on all questions and concerns.  These notes must be submitted at
least 24 hours prior to the actual review session.  They should be
neat notes, describing legitimate issues clear enough to be sent
as email, and organized for discussion (e.g. in a reasonable 
review order).
</P>

<P>
Each set of review notes will be graded on the basis of:
<ul>
	<li> 40% submitted 24 hours before scheduled review</li>
	<li> 30% the thoroughness of study to which they attest</li>
	<li> 10% how well articulated the issues are</li>
	<li> 10% all comments appropriate and within scope</li>
	<li> 10% issues reasonably organized for discussion</li>
</ul>

<P>
When your notes are eady for submission and grading:
<ul>
    <li> prepare your notes (ideally ASCII text in a file named <tt>notes_3d.txt</tt>).
	 If you are reviewing multiple components, it would be best if you combined
	 all of those notes into a single submission.</li>
    <li> put a <a href="#submission">standard submission prologue</a> on the front of the file</li>
    <li> up-load it for submission (each person must submit his/her own notes)</li>
</ul>
</P>
<a name="desc_report3">
<H3>P3C.2 Review and Report</h3>
</a>
<P>
You will conduct design reviews for each submitted
Specification/Design/Test Plan package.  The process will similar
to the architectural review ... but because this is 
simpler and you have already followed this process 
(for your architectural reviews) these
reviews will not be observed and graded.  
You also have much more latitude in these reviews:
<ul>
   <li> you can have the entire team review each person's proposal,
        or you can break it up into smaller groups for each review.</li>
   <li> you can have a scribe (who is not the owner of the module
        being reviewed) who writes and submits the report, or the
	module owner can act as the scribe and write up the report
	for his/her own review.</li>
</ul>
But each person must create and submit notes for (at least) one review, 
and must write up a report for one review.
</P>
<P>
As with the architectural review,
the report is not "meeting minutes".  Rather it is a distillation of key issues and
decisions.  It must contain:
<UL>
   <LI> the time, date, and attendees</li>
   <LI> the component to be reviewed</li>
   <LI> a clear summary of each important issue rased</li>
   <LI> a characterization as a defect, question, or issue</li>
   <LI> a characterization as must-fix, should-fix, or comment</li>
   <LI> a disposition for the entire proposal of
   <UL>
	<li> approved</li>
	<li> approved with required changes</li>
	<li> requires another meeting</li>
	<li> rejected (this cannot be made to work)</li>
   </UL>
   </LI>
</UL>
</P>
<P>
Each review report will be graded on the basis of:
<ul>
	<li> 10% form: time, place, project, attendees</li>
	<li> 20% scope of the report (just issues, no opinions)</li>

	<li> 50% clarity with which issues are presented</li>
	<li> 10% clear disposition for each issue</li>
	<li> 10% disposition of the entire project</li>
</ul>

<P>
When your review report is eady for submission and grading:
<ul>
    <li> create a <a href="#submission">standard submission prologue</a> 
         (followed by your review report or a URL to it), and
	 entitle it <tt>review_3d.txt</tt></li>
    <li> up-load it for submission (each person must submit his/her own review report)</li>
</ul>
</P>

<a name="desc_final3">
<H2>P3D Final Component Specifications (P3D.1), Design (P3D.2) and Test Plan (P3D.3)</h2>
<P>
Note: this is likely to be a relatively light week for project work.
      You would be well advised to use this opportunity to get an
      early start on the project 4 implementation ... so that you can
      have that out of the way in your (likely) otherwise busy final weeks.
</P>
</a>
<P>
It is likely that your design and test case development, and their
reviews will turn up issues that require changes to your specifications, 
design and test plan.  
Address those issues (by revising your specifications, design and test plan), 
document the changes that were made to address each, and get agreement from the 
reviewers that the issues have been satisfactorally addressed.
</P>
<P>
The primary parts of the final design submission are:
<UL>
   <LI> an updated component specification.</LI>
   <LI> an updated updated component design.</LI>
   <LI> an updated test plan.</LI>
</UL>
Each of thes should include a discussion of the issues that were 
discovered (since the preliminary submissions), and a summary of
the changes that have been made, and the reasons for each.
</P>
<P>
This submission be graded on the basis of:
<ul>
	<li> 15% quality and completness of the final specifications,
	         including responses to issues identified in the
		 reviews and grading of the earlier versions.</li>
	<li> 30% quality and completeness of the final design,
	         including responses to issues identified in the
		 reviews and grading of the earlier versions.</li>
	<li> 15% extent to which the final design meets the
	 	 <em>algorithmically interesting</em> requirement
		 (you will be implementing a significant amount of
		 non-trivial code).</li>
	<li> 25% quality and completeness of the final test plan,
	         including responses to issues identified in the
		 reviews and grading of the earlier versions.</li>
	<li> 15% extent to which the final test plan meets the
		 requirements for fully automated testability, and 
		 significant tests based on design/implementation 
		 rather than interface specifications.</li>
</ul>
</P>
<P>
When you have addressed all of the issues raised in your review and are
ready to submit your final component specifications, design and plan,
<ul>
    <li> prepare your spec (ideally ASCII text in a file named <tt>spec_3e.txt</tt>)</li>
    <li> prepare your design (ideally ASCII text in a file named <tt>design_3e.txt</tt>,
         or perhaps with some other language-specifc suffix)</li>
    <li> prepare your test plan (ideally ASCII text in a file named <tt>test_3e.txt</tt>)</li>
    <li> put a <a href="#submission">standard submission prologue</a> on the front of each file</li>
    <li> up-load them for submission (each person must submit his/her own files)</li>
</ul>
</P>

<a name="desc_3pm">
<H3>P3D.4 Post-Mortem Report</H3>
</a>
<P>
This project is a learning exercise, and one of the major ways
we learn is by analyzing past mistakes.  You will, as a team,
review all aspects of this project.  One of you will then 
summarize that process into a post-mortem analysis report.
</P>
<P>
A report, summarizing the key issues raised in your post-mortem,
and the conclusions you came to.  Your post-mortem discussion 
should include:
<ul>
	<li> architectural refinement and specification development.</li>
	<li> development of the component designs.</li>
	<li> development of the test plans.</li>
	<li> the review process and the resulting design/plan changes.</li>
	<li> the planning and ongoing management of these activities.</li>
	<li> the overall project as an educational exercise.</li>
</ul>
</P>
<P>
The submission and grading of Post Mortem reports is described
in the <a href="#postmortem">General Grading</a> information.
<P>
Make sure that you have kept your meeting minutes and management plan up-to-date on Github.
When you are ready to submit the Post-Mortem report (and management notes) for grading:
<ul>
    <li> prepare your report (ideally ASCII text in a file named <tt>postmortem_3.txt</tt>)</li>
    <li> put a <a href="#submission">standard submission prologue</a> on the front of it</li>
    <li> make sure that it also contains Github URLs for your status updates and management plans</li>
    <li> up-load it for submission (only one person on the team needs to do this)</li>
</ul>
</P>
<P>
This report be graded on the basis of:
<ul>
	<li> 50% whether or not you meaningfully discuss each of the required activities.</li>
	<li> 20% whether or not you identify all of the important incidents.</li>
	<li> 30% the extent to which you are able to derive and articulate useful lessons
	     (and good future advice) from those experiences.</li>
</ul>
</body></html>
